{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# <center> CS  4783/5783\n",
        "# <center> Fall 2022\n",
        "# <center> Assignment 2 â€“ Neural Networks \n",
        "# <center> Due: 10/07/2022 11:59 pm\n",
        "# <center> Submitted By: Haridas Das"
      ],
      "metadata": {
        "id": "NSyIu3o7ao0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Compile\n",
        "\n",
        "Click Runtime --> Restart Runtime\n",
        "\n",
        "This will clear out all variables from memory and let you start over."
      ],
      "metadata": {
        "id": "1Qjo9zl1cOSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1\n",
        "Derive the update rule and show how to train a 2-layer (1 hidden layer and 1 output layer) neural\n",
        "network with backpropagation for regression using the Mean Square Error loss. Assume that you\n",
        "are using the Sigmoid activation function for the hidden layer. Explain briefly how this is different\n",
        "from the update rule for the network trained for binary classification using log loss."
      ],
      "metadata": {
        "id": "_E-i5hSydK-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Slution of Problem 1"
      ],
      "metadata": {
        "id": "dPnh-5OREo6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Mean Square Error loss function \n",
        "\n",
        "$L= \\frac{1}{2N} \\sum_{i=1}^N (\\hat{y}-y)^2$, where $\\hat{y}$ and $y$ are the predicted and observed values respectively. \n",
        "\n",
        "Also hidden layer activation function $g(z)=\\frac{1}{1+e^{-z}}$, where $g'(z)=g(z)(1-g(z))$. \n",
        "\n",
        "Moreover, the  activation function $g(z)=z$ for the output layer. \n",
        "\n",
        "$Z_1=W_1x+b_1$, where $a_1=g(Z_1)$ and $Z_2=W_2 a_1+b_2$, where $a_2=g(Z_2)=\\hat{y}$. \n",
        "\n",
        "Now $\\frac{\\partial L}{\\partial W_2}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial W_2}\n",
        "=\\frac{2}{2N} \\sum_{i=1}^N (\\hat{y}-y) \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial w_2}=\\frac{1}{N}\\sum_{i=1}^N (\\hat{y}-y).1. a_1=\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}-y).a_1^T$, since $\\frac{\\partial \\hat{y}}{\\partial z_2} =\\frac{\\partial g{z_2}}{\\partial z_2}=1$ and $\\frac{\\partial z_2}{\\partial w_2}=a_1$.   \n",
        "\n",
        "Thus $\\frac{\\partial L}{\\partial W_2}=\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}-y).a_1^T$. By ignoring the sum, we basically have $\\frac{\\partial L}{\\partial W_2}= (\\hat{y}-y).a_1^T$. \n",
        "\n",
        "Now $\\frac{\\partial L}{\\partial W_1}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial W_1}=\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}-y) .1. W_2. a_1(1-a_1).x=\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}-y) W_2 a_1(1-a_1)x^T$. \n",
        "\n",
        "Hence $\\frac{\\partial L}{\\partial W_1}=\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}-y) W_2 a_1(1-a_1)x^T$. \n",
        "\n",
        "Again by ignoring the sum we have $\\frac{\\partial L}{\\partial W_1}= (\\hat{y}-y) W_2 a_1(1-a_1)x^T$.\n",
        "\n",
        "\n",
        "Similarly, $\\frac{\\partial L}{\\partial b_2}=(\\hat{y}-y) \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial b_2}=(\\hat{y}-y) .1. 1= (\\hat{y}-y)$, \n",
        "\n",
        "and \n",
        "\n",
        "$\\frac{\\partial L}{\\partial b_1}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial b_1}= (\\hat{y}-y) .1. W_2. a_1(1-a_1).x=(\\hat{y}-y) W_2 a_1(1-a_1)$. \n",
        "\n",
        "\n",
        "Finally, update the weights and bias by the following:\n",
        "\n",
        "$W_i=W_i -\\alpha \\frac{\\partial L}{\\partial W_i}$ and $b_i=b_i -\\alpha \\frac{\\partial L}{\\partial b_i}$\n",
        "\n",
        "where $i$ refers to the $i$th layer. We will repeat until convergence. "
      ],
      "metadata": {
        "id": "IwYPmA0k0aYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Classification using log loss\n",
        "\n",
        "We will see that it has exactly the same updated expression. \n",
        "\n",
        "We know \n",
        "$L=-[(1-y) log(1-\\hat{y})+ylog (\\hat{y})]$.  \n",
        "\n",
        "We use the activation function $g(z)=\\frac{1}{1+e^{-z}}$, where $g'(z)=g(z)(1-g(z))$ for hidden layer and output layer. \n",
        "\n",
        "Also, \n",
        "$Z_1=W_1x+b_1$, where $a_1=g(Z_1)$ and $Z_2=W_2 a_1+b_2$, where $a_2=g(Z_2)=\\hat{y}$. \n",
        "\n",
        "\n",
        "Now $\\frac{\\partial L}{\\partial W_2}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial W_2}\n",
        "=-[\\frac{(1-y)}{(1-\\hat{y})} \\frac{\\partial (1-a_2)}{\\partial a_2}  + \\frac{y}{\\hat{y}} \\frac{\\partial a_2}{\\partial a_2} ] g'(Z_2) a_1= -[\\frac{(1-y)}{(1-\\hat{y})} (-1)  + \\frac{y}{\\hat{y}} (1)] g(Z_2)(1-g(Z_2)) a_1=-[-\\frac{(1-y)}{(1-\\hat{y})}  + \\frac{y}{\\hat{y}}] a_2(1-a_2) a_1=[\\frac{(\\hat{y}-y)}{\\hat{y}(1-\\hat{y})}] \\hat{y}(1-\\hat{y}) a_1=(\\hat{y}-y).a_1^T$. \n",
        "\n",
        "\n",
        "Now $\\frac{\\partial L}{\\partial W_1}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial W_1}= -[ -\\frac{(1-y)}{(1-\\hat{y})}(-1) + \\frac{y}{\\hat{y}}] g'(Z_2) W_2 g'(Z_1) x=  \\frac{(\\hat{y}-y)}{\\hat{y}(1-\\hat{y})} \\hat{y} (1-\\hat{y}) W_2 a_1(1-a_1).x^T= (\\hat{y}-y) W_2 a_1(1-a_1).x^T$ \n",
        "\n",
        "\n",
        "Similarly, $\\frac{\\partial L}{\\partial b_2}=(\\hat{y}-y)$ and $\\frac{\\partial L}{\\partial b_1}=(\\hat{y}-y) W_2 a_1(1-a_1)$. \n",
        "\n",
        "Finally, update the weights and bias by the following:\n",
        "\n",
        "$W_i=W_i -\\alpha \\frac{\\partial L}{\\partial W_i}$ and $b_i=b_i -\\alpha \\frac{\\partial L}{\\partial b_i}$\n",
        "\n",
        "where $i$ refers to the $i$th layer. We will repeat until convergence. "
      ],
      "metadata": {
        "id": "KbRZZjYggBkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: \n",
        "\n",
        "We have derived the update rule and showed how to train a 2-layer (one hidden layer and one output layer) neural network with backpropagation for regression using the Mean Square Error loss and log loss. We used the Sigmoid activation function for the hidden layer and the output layer in the log loss function. On the contrary, we used the Sigmoid activation function for the hidden layer and the linear activation function for the output layer in the Mean Square Error loss function. Finally, we found that the Mean Square Error loss function and the log loss function's updated rules are the same. "
      ],
      "metadata": {
        "id": "cdHkBCghmnCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2\n",
        "For the given data on canvas, construct a neural network for the regression task. Your network\n",
        "must have 1 hidden layer and 1 output layer. Use sigmoid to be your activation function for the\n",
        "hidden layer(s). You can choose the number of neurons in each layer using your intuition.\n",
        "The data is already split to have your input data for training (X_train.csv) and testing\n",
        "(X_train.csv) and their corresponding target values Y_train.csv and Y_test.csv,\n",
        "respectively. You can load the data as follows:\n",
        "X_train = np.loadtxt(\"X_train.csv\")\n",
        "Implement the backpropagation algorithm and train your network until convergence.\n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "1. What is the activation function that you will choose for the output layer? Justify your answer\n",
        "briefly.\n",
        "2. How many neurons should there be in the output layer? Why?\n",
        "3. Report the average MSE loss and the accuracy.\n",
        "4. Plot the loss and accuracy as a function of the number of iterations.\n",
        "5. What is the effect of the learning rate on the training process? Vary the learning rate to be\n",
        "between 0.001 and 1.0 and plot the resulting accuracy as a function of learning rate.\n",
        "6. What is the effect of the number of neurons in the hidden layer? To answer this question,\n",
        "you will need to consider and answer the following:\n",
        "a. You will need to vary the number of neurons from 1 to 10. Does the update rule\n",
        "need to be changed/derived again? Why or why not?\n",
        "b. Report your observations by reporting the final loss and plotting the true labels and\n",
        "your predicted labels, along with a brief (2-3 lines) description.\n",
        "7. What is the effect of the activation functions in the network? Explore two different activation\n",
        "functions other than sigmoid such as tanh, linear, or ReLU.\n",
        "\n",
        "a. Will you need to change the update rule?\n",
        "\n",
        "b. What is the change that you need to make to achieve this experiment?\n",
        "\n",
        "c. Report your observations by reporting the final loss and plotting the true labels and your predicted labels, along with a brief (2-3 lines) description.\n"
      ],
      "metadata": {
        "id": "rxFkI6qE8YLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x9edoBkO9Zwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution of Problem 2:"
      ],
      "metadata": {
        "id": "x4mrBRtE9ybc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#import pandas as pd\n",
        "import random\n",
        "import math "
      ],
      "metadata": {
        "id": "bcVZkixOtCvr"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Drive and mount\n",
        "from google.colab import drive \n",
        "#load mounting drive to access the files on the google MyDrive. \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YvpB0K4wp0D",
        "outputId": "2fa8efe6-bac3-4390-92ae-d9dc2727d5af"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the training and test data as they are already split and given in Question 2.\n",
        "#X_train=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CS 5783 ML/Assignment2/X_train.csv') # training data\n",
        "# I like the following array\n",
        "X_train=np.loadtxt('/content/drive/MyDrive/Colab Notebooks/CS 5783 ML/Assignment2/X_train.csv') \n",
        "Y_train=np.loadtxt('/content/drive/MyDrive/Colab Notebooks/CS 5783 ML/Assignment2/Y_train.csv')\n",
        "X_test=np.loadtxt('/content/drive/MyDrive/Colab Notebooks/CS 5783 ML/Assignment2/X_test.csv')\n",
        "Y_test=np.loadtxt('/content/drive/MyDrive/Colab Notebooks/CS 5783 ML/Assignment2/Y_test.csv')"
      ],
      "metadata": {
        "id": "2gtesttnuIFx"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(X_train))\n",
        "print(np.shape(Y_train))\n",
        "print(np.shape(X_test))\n",
        "print(np.shape(Y_test))\n",
        "X_train[:,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b02JSPEE2mSn",
        "outputId": "27796fef-e7a0-4fc8-8d5d-a6ef5dc06f65"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2)\n",
            "(100,)\n",
            "(50, 2)\n",
            "(50,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.25189695, -0.39108897,  0.38603031,  0.75412143, -1.70608593,\n",
              "        1.00405615, -0.39115663,  0.14702677, -0.46875629, -0.80284317,\n",
              "       -0.1253808 , -1.08047146, -0.53247053, -2.05256292,  2.70684984,\n",
              "        0.64180551, -0.54725989,  0.18712452,  0.7207876 ,  1.13370299,\n",
              "       -0.8548599 , -0.44587278, -1.13664459, -0.982776  ,  0.90282586,\n",
              "        0.17800909,  2.15484644,  0.68120929,  0.16178119,  2.60596728,\n",
              "        0.86416488,  0.66631932,  0.33086562, -0.92860227, -0.97139257,\n",
              "       -0.85519604, -0.27951021,  0.23812696, -1.30648231,  2.02954418,\n",
              "       -0.28525677,  0.21423419, -0.75532534,  0.05366146, -1.3829201 ,\n",
              "        0.34353919,  0.16690464, -1.09769302,  0.67753511, -0.13484072,\n",
              "        0.80770591, -1.38537856,  0.55876941, -0.03443164, -0.17405654,\n",
              "       -0.92587426, -0.84807698,  0.40758645,  0.90796945,  0.39346076,\n",
              "       -0.29843574,  0.6610292 , -2.73646445, -0.550876  , -1.23561426,\n",
              "       -0.93323722, -0.72946634, -0.21988155,  0.30266545,  0.72902373,\n",
              "       -0.58038298, -0.04567648,  0.78331605,  2.78500967, -0.03116048,\n",
              "        0.80212937, -1.13381716, -0.15527575,  0.18869531,  0.19752399,\n",
              "        0.26620678, -0.03157914,  0.30563151,  1.75201442, -0.9932635 ,\n",
              "        1.58581239, -2.01816824, -0.13190614, -1.00518692,  0.23699563,\n",
              "       -1.39151906,  0.5868473 , -1.02957716,  0.83435452, -1.39828968,\n",
              "        1.60677986, -0.37651868,  2.16542112,  0.65111795, -1.9726051 ])"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Minimum value in the Y_train',np.min(Y_train))\n",
        "print('Maximum value in the Y_train',np.max(Y_train))\n",
        "print('Minimum value in the Y_test',np.min(Y_test))\n",
        "print('Maximum value in the Y_test',np.max(Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWfpT6ry2I8O",
        "outputId": "38c1a2e7-6677-4976-e9f9-742b67b77b00"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value in the Y_train -147.078116997524\n",
            "Maximum value in the Y_train 160.16900121190633\n",
            "Minimum value in the Y_test -178.6745345156375\n",
            "Maximum value in the Y_test 173.4091233043346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data frame information\n",
        "The training data has 100 examples or observation for training the model with 2 features. To test the model we have 50 examples. "
      ],
      "metadata": {
        "id": "czM85Y2qCDow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling\n",
        "We construct neural network (NN) for the regression. In the construction of NN, we use 1 hidden layer and 1 output layer. In addition, we use the sigmoid to be our activation function for the hidden layer. \n"
      ],
      "metadata": {
        "id": "IDDEpMGxdKzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 2(1)  \n",
        "To build the neural network model, we first want what activation function works for the output layer. As we have seen, the training and test data target variables are continuous. We, therefore, want to use the linear function $g(x)=ax+b$ as an activation function for the output layer, where $a=1$ and the bias term $b=0$. "
      ],
      "metadata": {
        "id": "PIWudBYuBpV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 2(2)  \n",
        "Because the target values are continuous, the output layer should contain one neuron."
      ],
      "metadata": {
        "id": "ot6CZSg5Y8Bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Development"
      ],
      "metadata": {
        "id": "BzugjAB8cY7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a loss function \n",
        "def loss_fun_mse(x,y,m,b):\n",
        "  y_predicted = model(a,m,b)\n",
        "  mse=(1/(2))* np.mean((y_predicted-y)**2)\n",
        "  return mse"
      ],
      "metadata": {
        "id": "8ajHQI6ghqlA"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data examples =n\n",
        "1. Suppose $X$ has $f$ features with examples $n$, so the input size $[n \\times f]$\n",
        "2. $Y$ be the output (target ) values $[n \\times 1]$\n",
        "3. Suppose $h_1$ is the number of neurons in layer 1 and  $Z_1}$ is the input of layer $1$.  \n",
        "3. At layer 1: Weight $W_1$ matrix sizs is number of neurons $\\times$ feature $f$ , i.e. , $h_1 \\times f$,  so the output $Z_1=W_1 \\times x+b_1$, where $b_1$ is bias with the size, $h_1 \\times 1$, and $a_1=g(Z_1)$ with the size $h_1 \\times 1$.\n",
        "4. Suppose the number of neurons in the output layer is $h_o$. As the output layer $Z_2=W_2 \\times a_1+b_2$, so the size of the weight $W_2$ euals to $h_o \\times h_1$, where $h_1$ number of neurons in the previous layer, or the input size of $a_2=g(Z_2)$ (number of neurons $\\times$ 1.) \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HSPVlZn5ydBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iters=5\n",
        "iter_list = [] \n",
        "for i in range(num_iters):\n",
        "  iter_list.append(i)\n",
        "print(iter_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puFk8WcyK92K",
        "outputId": "07cec7aa-063d-4f2b-9355-f96ce58af698"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Code"
      ],
      "metadata": {
        "id": "9M-IlMqMmQMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import legend\n",
        "##final revised  \n",
        "\n",
        "#creating neural networks for two layers \n",
        "\n",
        "#Setting the hyperparameters \n",
        "N=X_train.shape[0]\n",
        "h_1 = 3 # layer1_neurons\n",
        "learning_rate = 0.001\n",
        "num_iters = 100\n",
        "h_o = 1              # output_layer_neurons\n",
        "\n",
        "f = X_train.shape[1] # number of feature in the traing and test data\n",
        "\n",
        "# make the iteration into a list\n",
        "iter_list = [] \n",
        "for i in range(num_iters):\n",
        "  iter_list.append(i)\n",
        "\n",
        "# Randomly initialize weight w1,w2 and bias terms b1, b2\n",
        "w1 = np.random.rand(h_1,f)\n",
        "b1 = np.random.rand(h_1,1)\n",
        "w2 = np.random.rand(h_o,h_1) \n",
        "b2 = np.random.rand(h_o,1)\n",
        "\n",
        "# Define the training set and test set \n",
        "x=X_train\n",
        "y=Y_train\n",
        "\n",
        "# Define the empty set to strore the many updates \n",
        "\n",
        "mse_list = []\n",
        "avg_mse_list = []\n",
        "mse_list_alt = []\n",
        "accuracy_list = []\n",
        "avg_mse_list = []\n",
        "truep_list = []\n",
        "truenp_list = []\n",
        "\n",
        "\n",
        "# Record weight and bias values \n",
        "w1_list = []\n",
        "w2_list = []\n",
        "b1_list= []\n",
        "b2_list = []\n",
        "\n",
        "#Working for the generalization for the neural networks\n",
        "f=x.shape[1]\n",
        "\n",
        "# Define the sigmoid function for the hidden layer\n",
        "def sigmoid_fun(z):\n",
        "  g1=1/(1+np.exp(-z))\n",
        "  return g1\n",
        "\n",
        "# Define the activation function for the output layer \n",
        "def linear_fun(z,a,b):\n",
        "  g2=a*z+b\n",
        "  return g2\n",
        "\n",
        "for i in range(x.shape[0]):\n",
        "  y_pred = []\n",
        "  mse_j_collect = []\n",
        "  #avg_mse_list = []\n",
        "  #truep_list = []\n",
        "  #truenp_list = []\n",
        "  #accuracy_list = []\n",
        "  for j in range(x.shape[0]):\n",
        "    z1 = np.dot(w1,x[j].reshape(f,1))+b1\n",
        "    a1 = sigmoid_fun(z1)\n",
        "    z2 = np.dot(w2,a1)+b2\n",
        "    a2 = linear_fun(z2,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "    # want to store this predicted score in the y_pred\n",
        "    y_pred.append(a2)\n",
        "\n",
        "\n",
        "    # calculate the accuaracy\n",
        "    #if y[j]== a2 or np.round(a2,0):\n",
        "     # truep_list.append(1)\n",
        "    #else:\n",
        "     # truenp_list.append(1)\n",
        "       \n",
        "    #y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten() is also useful\n",
        "    #Using Backprop update the weights and bias terms \n",
        "    # Backward pass\n",
        "    dL_dw = (a2-y[j])\n",
        "  \n",
        "    # update weights\n",
        "    w2 = w2-(learning_rate)*dL_dw*a1.T  # check the matrix \n",
        "    w1 = w1-(learning_rate)*dL_dw*np.dot(w2,a1)*np.dot(1-a1,x[1].reshape(2,1).T)\n",
        "    b2 = b2-(learning_rate)*dL_dw  # check the matrix \n",
        "    b1 = b1-(learning_rate)*dL_dw*w2.T*np.dot(a1.T,1-a1)\n",
        "\n",
        "    # mse calculate\n",
        "    mse_j_ind=(a2-y[j])**2\n",
        "    mse_j_collect.append(mse_j_ind)\n",
        "\n",
        "\n",
        "  d=sum(mse_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "  mse_list_alt.append(d)\n",
        "  w1_list.append(w1)\n",
        "  w2_list.append(w2)\n",
        "  b1_list.append(b1)\n",
        "  b2_list.append(b2)\n",
        "\n",
        "  # update the y_pred in each epoch \n",
        "  y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten()\n",
        "  mse_ = np.sum((y_pred-y)**2)/(2*x.shape[0])\n",
        "  mse_list.append(mse_)\n",
        "  #accuary_ind_list=len(truep_list)/100\n",
        "  #accuracy_list.append(accuary_ind_list)\n",
        "avg_mse_list_alt=np.sum(mse_list_alt)/x.shape[0]\n",
        "print('Alternative option: Average MSE on training data',avg_mse_list_alt)\n",
        "avg_mse_list=np.sum(mse_list)/x.shape[0]\n",
        "print('Average MSE on training data',avg_mse_list) # Both ways give us the same average MSE on training data\n",
        "plt.plot(iter_list, mse_list)\n",
        "plt.xlabel('Number of iteration')\n",
        "plt.ylabel('Mean square Loss')\n",
        "plt.legend()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "BbrJ-UhesFGE",
        "outputId": "9d2fecb8-8223-4cc8-ae79-87f38a345316"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alternative option: Average MSE on training data 1013.6618026065149\n",
            "Average MSE on training data 1013.6618026065152\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9fb313ead0>"
            ]
          },
          "metadata": {},
          "execution_count": 288
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2(3)  and Problem 2(4) "
      ],
      "metadata": {
        "id": "Jnp_P_GNixdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import legend\n",
        "##final revised  \n",
        "\n",
        "#creating neural networks for two layers \n",
        "\n",
        "#Setting the hyperparameters \n",
        "N=X_train.shape[0]\n",
        "h_1 = 10 # layer1_neurons\n",
        "learning_rate = 0.001\n",
        "num_iters = 100\n",
        "h_o = 1              # output_layer_neurons\n",
        "f = X_train.shape[1] # number of feature in the traing and test data\n",
        "\n",
        "# make the iteration into a list\n",
        "iter_list = [] \n",
        "for i in range(num_iters):\n",
        "  iter_list.append(i)\n",
        "\n",
        "# Randomly initialize weight w1,w2 and bias terms b1, b2\n",
        "w1 = np.random.rand(h_1,f)\n",
        "b1 = np.random.rand(h_1,1)\n",
        "w2 = np.random.rand(h_o,h_1) \n",
        "b2 = np.random.rand(h_o,1)\n",
        "\n",
        "# Define the training set and test set \n",
        "x=X_train\n",
        "y=Y_train\n",
        "\n",
        "# Define the empty set to strore the many updates \n",
        "\n",
        "mse_list = []\n",
        "avg_mse_list = []\n",
        "mse_list_alt = []\n",
        "accuracy_list = []\n",
        "avg_mse_list = []\n",
        "truep_list = []\n",
        "truenp_list = []\n",
        "\n",
        "\n",
        "# Record weight and bias values \n",
        "w1_list = []\n",
        "w2_list = []\n",
        "b1_list= []\n",
        "b2_list = []\n",
        "a2_new_list = []\n",
        "#Working for the generalization for the neural networks\n",
        "f=x.shape[1]\n",
        "\n",
        "# Define the sigmoid function for the hidden layer\n",
        "def sigmoid_fun(z):\n",
        "  g1=1/(1+np.exp(-z))\n",
        "  return g1\n",
        "\n",
        "# Define the activation function for the output layer \n",
        "def linear_fun(z,a,b):\n",
        "  g2=a*z+b\n",
        "  return g2\n",
        "\n",
        "for i in range(x.shape[0]):\n",
        "  y_pred = []\n",
        "  mse_j_collect = []\n",
        "  #avg_mse_list = []\n",
        "  #truep_list = []\n",
        "  #truenp_list = []\n",
        "  #accuracy_list = []\n",
        "  for j in range(x.shape[0]):\n",
        "    z1 = np.dot(w1,x[j].reshape(f,1))+b1\n",
        "    a1 = sigmoid_fun(z1)\n",
        "    z2 = np.dot(w2,a1)+b2\n",
        "    a2 = linear_fun(z2,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "    # want to store this predicted score in the y_pred\n",
        "    y_pred.append(a2)\n",
        "\n",
        "\n",
        "    # calculate the accuaracy\n",
        "    #if y[j]== a2 or np.round(a2,0):\n",
        "     # truep_list.append(1)\n",
        "    #else:\n",
        "     # truenp_list.append(1)\n",
        "       \n",
        "    #y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten() is also useful\n",
        "    #Using Backprop update the weights and bias terms \n",
        "    # Backward pass\n",
        "    dL_dw = (a2-y[j])\n",
        "  \n",
        "    # update weights\n",
        "    w2 = w2-(learning_rate)*dL_dw*a1.T  # check the matrix \n",
        "    w1 = w1-(learning_rate)*dL_dw*np.dot(w2,a1)*np.dot(1-a1,x[1].reshape(2,1).T)\n",
        "    b2 = b2-(learning_rate)*dL_dw  # check the matrix \n",
        "    b1 = b1-(learning_rate)*dL_dw*w2.T*np.dot(a1.T,1-a1)\n",
        "\n",
        "    # mse calculate\n",
        "    mse_j_ind=(a2-y[j])**2\n",
        "    mse_j_collect.append(mse_j_ind)\n",
        "\n",
        "\n",
        "  d=sum(mse_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "  mse_list_alt.append(d)\n",
        "  w1_list.append(w1)\n",
        "  w2_list.append(w2)\n",
        "  b1_list.append(b1)\n",
        "  b2_list.append(b2)\n",
        "\n",
        "  # update the y_pred in each epoch \n",
        "  y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten()\n",
        "  mse_ = np.sum((y_pred-y)**2)/(2*x.shape[0])\n",
        "  mse_list.append(mse_)\n",
        "  #accuary_ind_list=len(truep_list)/100\n",
        "  #accuracy_list.append(accuary_ind_list)\n",
        "\n",
        "\n",
        "# avg mse calculation using the training data\n",
        "avg_mse_list_alt=np.sum(mse_list_alt)/x.shape[0]\n",
        "print('Alternative option: Average MSE on training data',avg_mse_list_alt)\n",
        "avg_mse_list=np.sum(mse_list)/x.shape[0]\n",
        "print('Average MSE on training data',avg_mse_list) # Both ways give us the same average MSE on training data\n",
        "plt.plot(iter_list, mse_list)\n",
        "plt.xlabel('Number of iteration')\n",
        "plt.ylabel('Mean square Loss')\n",
        "plt.title('Number of iteration Vs. Mean square Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# using the test data we will calculate the errorn on the \n",
        "\n",
        "# To do this, we choose the weights that do well on your training set and can generalize well. \n",
        "## This usually is the weights producing the least MSE on training data. Ref: Dr. Sathya\n",
        "#For future reference, this is the default for any and all ML algorithms. \n",
        "## We should always pick the model with the best training accuracy and error. Ref: Dr. Sathya\n",
        "#First, we will see how find the best weights that gives least MSE and best accuracy on training data fir the model predictors\n",
        "# Perhaps, we don't compute the accuracy so we only consider least MSE\n",
        "\n",
        "minimum = mse_list[0]\n",
        "index = 0\n",
        "for i in range(len(mse_list)):\n",
        "  if mse_list[i] < minimum:\n",
        "    minimum = mse_list[i]\n",
        "    index= i\n",
        "print('Least MSE on training data is', minimum,'at index',index) \n",
        "\n",
        "\n",
        "# Fwd Pass with new weights\n",
        "y_pred_train_new = []\n",
        "mse_train_new_list =[]\n",
        "mse_train_new_ind = []\n",
        "a2_train_new_list = []\n",
        "mse_new_j_collect = []\n",
        "mse_new_list_alt = []\n",
        "w1_new=w1_list[index]\n",
        "w2_new=w2_list[index]\n",
        "b1_new=b1_list[index]\n",
        "b2_new=b2_list[index]\n",
        "for j in range (100):\n",
        "  z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "  a1_new = sigmoid_fun(z1_new)\n",
        "  z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "  a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "  a2_new_list.append(a2_new)\n",
        "  y_pred_train_new.append(a2)\n",
        "  \n",
        "# mse calculate\n",
        "  mse_new_j_ind=(a2_new-y[j])**2\n",
        "  mse_new_j_collect.append(mse_new_j_ind)\n",
        "\n",
        "\n",
        "d=sum(mse_new_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "mse_new_list_alt.append(d)\n",
        "\n",
        "# avg mse calculation using the training data\n",
        "avg_mse_new_list_alt=np.sum(mse_new_list_alt)/x.shape[0]\n",
        "print('Average MSE on training data in the new prediction',avg_mse_new_list_alt)\n",
        "#plt.plot(iter_list, mse_new_j_collect)\n",
        "#plt.xlabel('Number of iteration')\n",
        "#plt.ylabel('Mean square Loss')\n",
        "#plt.title('Number of iteration Vs. Mean square Loss')\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Initial Prediction\", y_pred, \"New Prediction:\", a2_new_list, \"Actual:\", y)\n",
        "#print(\"Initial Prediction\",        \"New Prediction:\",       \"Actual:\")\n",
        "#print( y_pred, a2_new_list, y)\n",
        "\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"New Prediction:\",a2_new_list)\n",
        "print(\"Actual:\",y)\n",
        "\n",
        "# alternative way to print not perfect for the moment\n",
        "#for a,b,c in zip(y_pred[::100],a2_new_list[1::100],y[2::100]):\n",
        "  #print '{:<30}{:<30}{:<}'.format(a,b,c)\n",
        "\n",
        "\n",
        "\n",
        "# Analysis on the Test data \n",
        "#X_test\n",
        "#Y_test\n",
        "mse_test_list =[]\n",
        "mse_list_ind = []\n",
        "a2_test_list = []\n",
        "# Fwd Pass with new weights\n",
        "w1_new=w1_list[index]\n",
        "w2_new=w2_list[index]\n",
        "b1_new=b1_list[index]\n",
        "b2_new=b2_list[index]\n",
        "for j in range (X_test.shape[0]):\n",
        "  z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "  a1_new = sigmoid_fun(z1_new)\n",
        "  z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "  a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "  a2_test_list.append(a2_new)\n",
        "  mse_j_test=(a2-y[j])**2\n",
        "  mse_list_ind.append(mse_j_test)\n",
        "\n",
        "dd=sum(mse_list_ind)/(2*X_test.shape[0]) # mean square error in each iteration\n",
        "mse_test_list.append(dd)\n",
        "avg_mse_test_list=np.sum(mse_test_list)/X_test.shape[0]\n",
        "print('Average MSE on test data',avg_mse_test_list) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZPh2Tyx1xPaf",
        "outputId": "d3099337-4a64-485f-894d-846304ed5030"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: overflow encountered in exp\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alternative option: Average MSE on training data 814.5700194090489\n",
            "Average MSE on training data 814.5700194090487\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Least MSE on training data is 434.9092964255389 at index 82\n",
            "Average MSE on training data in the new prediction 5.230041038724463\n",
            "Initial Prediction: [-64.3023313   98.45788626  98.8169921  -64.210184   -64.19958219\n",
            "  53.67252867 104.72643291 -64.23029538  92.4451753  -29.33465781\n",
            " -64.22138894  93.66120439  93.40093617  23.28676624   1.81842171\n",
            " -64.19309448  26.8189988  -64.18410799 -64.1708665   23.34875319\n",
            " 111.67811169  90.68708973  23.13452402  93.48151675 -64.18314933\n",
            " -64.15626835 -64.13569238  86.78282628  56.6633275   23.35139049\n",
            " -64.10276504 -64.14711155 -64.12033014 111.82948421 -64.12269933\n",
            " 111.86608581 100.91485685 111.14239362 -64.16858174 -64.21810214\n",
            " 111.73908954 -63.51974057 -55.63041207 -64.1145503  109.66527672\n",
            "  23.55625031  23.49951293 -64.22475511 -33.84433561  23.74339018\n",
            " -26.75796943 -64.22228086  94.36559642  86.10072163  90.79904784\n",
            "  93.38292725  82.47905214  67.88745595  22.92062335 -64.34390685\n",
            "  22.92486394 -64.34936005 -64.30127717  74.35873126 -64.32332381\n",
            "  74.20682127 -64.3106517   22.81125252  74.15419048 -64.2199537\n",
            "  93.466339    -7.20784252 -64.21270478  89.51049159  93.63828007\n",
            " -64.19284164  80.87200339 -64.28292714 -64.28919849  93.64578745\n",
            "  23.55698647  45.33386893  23.51768772  23.58485635  23.99361249\n",
            "  40.98610375 105.71961255 -64.22883864 -64.2572416   23.36341802\n",
            "  23.75842377 -64.26792049 -64.31633887 -64.19858957  23.14467203\n",
            " -64.34218881  22.9488628  -64.36223527 -64.3835728  -64.34116033]\n",
            "New Prediction: [array([[-61.03175883]]), array([[72.50046315]]), array([[91.08328423]]), array([[-62.08574791]]), array([[-62.07337123]]), array([[30.05097224]]), array([[44.43551987]]), array([[-60.61972018]]), array([[33.42170732]]), array([[-28.75698579]]), array([[-62.08779626]]), array([[44.77777073]]), array([[102.91594324]]), array([[28.87310953]]), array([[-17.71762278]]), array([[-62.0947301]]), array([[29.23537184]]), array([[-62.03200155]]), array([[-62.09776561]]), array([[29.18191811]]), array([[67.22191811]]), array([[29.49825331]]), array([[28.95840361]]), array([[106.67082318]]), array([[-61.94842014]]), array([[-61.85732907]]), array([[-62.09354344]]), array([[31.4628292]]), array([[29.67724099]]), array([[29.02457535]]), array([[-62.09782307]]), array([[-61.85142109]]), array([[-62.09691351]]), array([[76.99874052]]), array([[-62.09775959]]), array([[62.79513668]]), array([[45.14148436]]), array([[86.03962341]]), array([[-62.09778829]]), array([[-62.09779064]]), array([[99.5244939]]), array([[-47.01493175]]), array([[-40.06658344]]), array([[-62.09782468]]), array([[64.81759326]]), array([[28.7587268]]), array([[28.85368493]]), array([[-62.09782384]]), array([[-25.58519085]]), array([[29.20567889]]), array([[-23.99052882]]), array([[-62.09623379]]), array([[31.93018637]]), array([[29.29960522]]), array([[29.76957439]]), array([[90.52476607]]), array([[29.48570391]]), array([[29.36125237]]), array([[29.18799559]]), array([[-61.95282063]]), array([[27.59075661]]), array([[-59.26282031]]), array([[-60.37406812]]), array([[40.55072509]]), array([[-61.45845626]]), array([[45.95460076]]), array([[-61.54938021]]), array([[29.17753776]]), array([[59.83178559]]), array([[-62.09357428]]), array([[49.20130972]]), array([[-20.55007951]]), array([[-62.07351282]]), array([[36.00569589]]), array([[84.77763639]]), array([[-62.09782383]]), array([[29.91008554]]), array([[-62.09306926]]), array([[-62.04987754]]), array([[104.58530132]]), array([[29.19410278]]), array([[29.26130817]]), array([[28.98592459]]), array([[29.20226701]]), array([[29.19123931]]), array([[29.24522941]]), array([[45.63967301]]), array([[-62.09772766]]), array([[-61.33563056]]), array([[28.87484938]]), array([[29.1965014]]), array([[-62.09782342]]), array([[-62.09775297]]), array([[-47.54510216]]), array([[25.03500184]]), array([[-62.09779031]]), array([[29.15252738]]), array([[-62.0978193]]), array([[-58.72906453]]), array([[-60.73798752]])]\n",
            "Actual: [-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
            " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
            "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
            "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
            "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
            "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
            " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
            "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
            " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
            "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
            "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
            "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
            "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
            "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
            "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
            "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
            " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
            "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
            " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
            "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
            "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
            "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
            " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
            " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
            "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n",
            "Average MSE on test data 123.3725740396072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZyANkp9uguTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2(3)\n",
        "\n",
        "Average MSE on training data 814.5700194090487\n",
        "\n",
        "Average MSE on test data 123.3725740396072.\n",
        "\n",
        "# Problem 2(4) : \n",
        "We plot the MSE loss function vs. the number of iterations. The figure and the results show that initially, the loss is high, but after several iterations, the loss becomes low and constant for both datasets. Moreover, average MSE on test data give us the low average MSE on test data compare to traing data.\n",
        "In this experiment, I used ten neurons in the hidden layer and one neuron in the output layer, and the learning rate=0.001 also used backpropagation 100 times. "
      ],
      "metadata": {
        "id": "2dyTBdmjAzym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2(5):"
      ],
      "metadata": {
        "id": "WG8b9i-ijqdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learningRateList=[0.001,0.003,0.01,0.03,0.1,0.3,1.0]\n",
        "#Setting the hyperparameters \n",
        "N=X_train.shape[0]\n",
        "h_1 = 10 # layer1_neurons\n",
        "learning_rate = 0.001\n",
        "num_iters = 100\n",
        "h_o = 1              # output_layer_neurons\n",
        "f = X_train.shape[1] # number of feature in the traing and test data\n",
        "\n",
        "# make the iteration into a list\n",
        "iter_list = [] \n",
        "for i in range(num_iters):\n",
        "  iter_list.append(i)\n",
        "\n",
        "# Randomly initialize weight w1,w2 and bias terms b1, b2\n",
        "w1 = np.random.rand(h_1,f)\n",
        "b1 = np.random.rand(h_1,1)\n",
        "w2 = np.random.rand(h_o,h_1) \n",
        "b2 = np.random.rand(h_o,1)\n",
        "\n",
        "# Define the training set and test set \n",
        "x=X_train\n",
        "y=Y_train\n",
        "\n",
        "# Define the empty set to strore the many updates \n",
        "\n",
        "mse_list = []\n",
        "avg_mse_list = []\n",
        "mse_list_alt = []\n",
        "accuracy_list = []\n",
        "avg_mse_list = []\n",
        "truep_list = []\n",
        "truenp_list = []\n",
        "\n",
        "\n",
        "# Record weight and bias values \n",
        "w1_list = []\n",
        "w2_list = []\n",
        "b1_list= []\n",
        "b2_list = []\n",
        "a2_new_list = []\n",
        "#Working for the generalization for the neural networks\n",
        "f=x.shape[1]\n",
        "\n",
        "# Define the sigmoid function for the hidden layer\n",
        "def sigmoid_fun(z):\n",
        "  g1=1/(1+np.exp(-z))\n",
        "  return g1\n",
        "\n",
        "# Define the activation function for the output layer \n",
        "def linear_fun(z,a,b):\n",
        "  g2=a*z+b\n",
        "  return g2\n",
        "\n",
        "\n",
        "for k in range(len(learningRateList)):\n",
        "  learning_rate=learningRateList[k]\n",
        "  for i in range(x.shape[0]):\n",
        "    y_pred = []\n",
        "    mse_j_collect = []\n",
        "    #avg_mse_list = []\n",
        "    #truep_list = []\n",
        "    #truenp_list = []\n",
        "    #accuracy_list = []\n",
        "    for j in range(x.shape[0]):\n",
        "      z1 = np.dot(w1,x[j].reshape(f,1))+b1\n",
        "      a1 = sigmoid_fun(z1)\n",
        "      z2 = np.dot(w2,a1)+b2\n",
        "      a2 = linear_fun(z2,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "      # want to store this predicted score in the y_pred\n",
        "      y_pred.append(a2)\n",
        "\n",
        "\n",
        "      # calculate the accuaracy\n",
        "      #if y[j]== a2 or np.round(a2,0):\n",
        "      # truep_list.append(1)\n",
        "      #else:\n",
        "      # truenp_list.append(1)\n",
        "       \n",
        "      #y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten() is also useful\n",
        "      #Using Backprop update the weights and bias terms \n",
        "      # Backward pass\n",
        "      dL_dw = (a2-y[j])\n",
        "  \n",
        "      # update weights\n",
        "      w2 = w2-(learning_rate)*dL_dw*a1.T  # check the matrix \n",
        "      w1 = w1-(learning_rate)*dL_dw*np.dot(w2,a1)*np.dot(1-a1,x[1].reshape(2,1).T)\n",
        "      b2 = b2-(learning_rate)*dL_dw  # check the matrix \n",
        "      b1 = b1-(learning_rate)*dL_dw*w2.T*np.dot(a1.T,1-a1)\n",
        "\n",
        "      # mse calculate\n",
        "      mse_j_ind=(a2-y[j])**2\n",
        "      mse_j_collect.append(mse_j_ind)\n",
        "\n",
        "  d=sum(mse_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "  mse_list_alt.append(d)\n",
        "  w1_list.append(w1)\n",
        "  w2_list.append(w2)\n",
        "  b1_list.append(b1)\n",
        "  b2_list.append(b2)\n",
        "\n",
        "    # update the y_pred in each epoch \n",
        "  y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten()\n",
        "  mse_ = np.sum((y_pred-y)**2)/(2*x.shape[0])\n",
        "  mse_list.append(mse_)\n",
        "  #accuary_ind_list=len(truep_list)/100\n",
        "  #accuracy_list.append(accuary_ind_list)\n",
        "\n",
        "\n",
        "   # avg mse calculation using the training data\n",
        "   #avg_mse_list_alt=np.sum(mse_list_alt)/x.shape[0]\n",
        "   #print('Alternative option: Average MSE on training data',avg_mse_list_alt)\n",
        "   #avg_mse_list=np.sum(mse_list)/x.shape[0]\n",
        "   #print('Average MSE on training data',avg_mse_list) # Both ways give us the same average MSE on training data\n",
        "   #plt.plot(iter_list, mse_list)\n",
        "   #plt.xlabel('Number of iteration')\n",
        "   #plt.ylabel('Mean square Loss')\n",
        "   #plt.title('Number of iteration Vs. Mean square Loss')\n",
        "   #plt.legend()\n",
        "   #plt.show()\n",
        "\n",
        "\n",
        "# using the test data we will calculate the errorn on the \n",
        "\n",
        "# To do this, we choose the weights that do well on your training set and can generalize well. \n",
        "## This usually is the weights producing the least MSE on training data. Ref: Dr. Sathya\n",
        "#For future reference, this is the default for any and all ML algorithms. \n",
        "## We should always pick the model with the best training accuracy and error. Ref: Dr. Sathya\n",
        "#First, we will see how find the best weights that gives least MSE and best accuracy on training data fir the model predictors\n",
        "# Perhaps, we don't compute the accuracy so we only consider least MSE\n",
        "\n",
        "  minimum = mse_list[0]\n",
        "  index = 0\n",
        "  for i in range(len(mse_list)):\n",
        "   if mse_list[i] < minimum:\n",
        "     minimum = mse_list[i]\n",
        "     index= i\n",
        "  print('Least MSE on training data is', minimum,'at index',index) \n",
        "\n",
        "\n",
        "  # Fwd Pass with new weights\n",
        "  y_pred_train_new = []\n",
        "  mse_train_new_list =[]\n",
        "  mse_train_new_ind = []\n",
        "  a2_train_new_list = []\n",
        "  mse_new_j_collect = []\n",
        "  mse_new_list_alt = []\n",
        "  w1_new=w1_list[index]\n",
        "  w2_new=w2_list[index]\n",
        "  b1_new=b1_list[index]\n",
        "  b2_new=b2_list[index]\n",
        "  for j in range (100):\n",
        "    z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "    a1_new = sigmoid_fun(z1_new)\n",
        "    z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "    a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "    a2_new_list.append(a2_new)\n",
        "    y_pred_train_new.append(a2)\n",
        "  \n",
        "    # mse calculate\n",
        "  mse_new_j_ind=(a2_new-y[j])**2\n",
        "  mse_new_j_collect.append(mse_new_j_ind)\n",
        "\n",
        "\n",
        "  d=sum(mse_new_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "  mse_new_list_alt.append(d)\n",
        "\n",
        "  # avg mse calculation using the training data\n",
        "  avg_mse_new_list_alt=np.sum(mse_new_list_alt)/x.shape[0]\n",
        "  print('Average MSE on training data in the new prediction',avg_mse_new_list_alt)\n",
        "  #plt.plot(iter_list, mse_new_j_collect)\n",
        "  #plt.xlabel('Number of iteration')\n",
        "  #plt.ylabel('Mean square Loss')\n",
        "  #plt.title('Number of iteration Vs. Mean square Loss')\n",
        "  #plt.legend()\n",
        "  #plt.show()\n",
        "  print(\"Initial Prediction:\", y_pred)\n",
        "  print(\"New Prediction:\",a2_new_list)\n",
        "  print(\"Actual:\",y)\n",
        "\n",
        "  # Analysis on the Test data \n",
        "  #X_test\n",
        "  #Y_test\n",
        "  mse_test_list =[]\n",
        "  mse_list_ind = []\n",
        "  a2_test_list = []\n",
        "  # Fwd Pass with new weights\n",
        "  w1_new=w1_list[index]\n",
        "  w2_new=w2_list[index]\n",
        "  b1_new=b1_list[index]\n",
        "  b2_new=b2_list[index]\n",
        "  for j in range (X_test.shape[0]):\n",
        "    z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "    a1_new = sigmoid_fun(z1_new)\n",
        "    z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "    a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "    a2_test_list.append(a2_new)\n",
        "    mse_j_test=(a2-y[j])**2\n",
        "    mse_list_ind.append(mse_j_test)\n",
        "\n",
        "  dd=sum(mse_list_ind)/(2*X_test.shape[0]) # mean square error in each iteration\n",
        "  mse_test_list.append(dd)\n",
        "  avg_mse_test_list=np.sum(mse_test_list)/X_test.shape[0]\n",
        "  print('Average MSE on test data',avg_mse_test_list) \n",
        "\n",
        "epoch = 100\n",
        "num_epoch = np.arange(epoch)\n",
        "plt.plot(num_epoch, np.array(mse_test_list[0:100]).reshape(epoch,1), color = 'red')\n",
        "plt.plot(num_epoch, np.array(mse_test_list[100:200]).reshape(epoch,1), color = 'green')\n",
        "plt.plot(num_epoch, np.array(mse_test_list[200:300]).reshape(epoch,1), color = 'blue')\n",
        "plt.plot(num_epoch, np.array(mse_test_list[300:400]).reshape(epoch,1), color = 'black')\n",
        "plt.plot(num_epoch, np.array(mse_test_list[400:500]).reshape(epoch,1), color = 'yellow')\n",
        "plt.plot(num_epoch, np.array(mse_test_list[500:600]).reshape(epoch,1), color = 'orange')\n",
        "plt.plot(num_epoch, np.array(mse_test_list[600:700]).reshape(epoch,1), color = 'purple')\n",
        "plt.legend((\"0.001 learning\", \"0.003 learning\", \"0.01 learning\", \"0.03 learning\", \"0.1 learning\",\"0.3 learning\", \"1.0 learning\"))\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Number of Iteration')\n",
        "plt.title('RMSE Vs Iteration for different learning rate ')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UGmjn47DOA0o",
        "outputId": "ba8ba312-5759-405b-bb76-7e4790d7d2f3"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Least MSE on training data is 897.0076637739661 at index 0\n",
            "Average MSE on training data in the new prediction 0.040336735401811506\n",
            "Initial Prediction: [-31.94945936 121.70042731 143.67949267 -31.96586673 -31.9986923\n",
            "  68.31066584  68.33835646 -32.09057164  68.35848811 -32.10880986\n",
            " -32.07870545  68.26238973 143.49990485 -32.14718568 -32.12259436\n",
            " -31.961198   -32.02379097 -31.89072713 -31.92717661 -32.0404615\n",
            " 121.68940604  68.42540791 -31.95721367 143.61607934 -31.87634549\n",
            " -31.88474169 -31.91298725  68.37185956  68.39794529 -31.95116938\n",
            " -31.78373018 -31.93688094 -31.94495513 121.43154055 -32.02602801\n",
            " 110.45296553  68.05697868 121.27107083 -32.12617556 -32.2997909\n",
            " 143.31716026 -32.37452716 -32.34034509 -32.31430279 120.99015257\n",
            " -32.59120901 -32.49939253 -32.39632459 -32.60821338 -32.53925193\n",
            " -32.42308024 -32.33581066  67.81186828  48.92670903  67.832713\n",
            " 120.92083252  67.79881453  56.85369862 -32.52595555 -32.38886366\n",
            " -32.41009248 -32.34499832 -32.31194681  67.74202411 -32.39512511\n",
            "  67.68184167 -32.44576454 -32.48421612  67.73656108 -32.27906258\n",
            "  68.02992606 -32.30467046 -32.23661774  68.16502338 143.76053948\n",
            " -32.2023813   68.21489802 -32.42342496 -32.50091385 143.20456252\n",
            " -32.51524899 -29.43160946 -32.24546786 -32.15377922 -31.98626415\n",
            " -31.90525257  68.85575663 -31.73326421 -31.854308   -31.90170924\n",
            " -31.80724956 -31.71970675 -31.87389695 -32.03329599 -31.98333426\n",
            " -31.95298057 -32.04541103 -31.94835807 -32.06021729 -32.02983993]\n",
            "New Prediction: [array([[-32.09266106]]), array([[121.60295793]]), array([[136.8458323]]), array([[-32.09403208]]), array([[-32.08406455]]), array([[68.37047457]]), array([[68.37327581]]), array([[-32.09433439]]), array([[68.37428306]]), array([[-32.09333155]]), array([[-32.09050937]]), array([[68.37535431]]), array([[143.73087531]]), array([[-32.09052273]]), array([[-32.10545436]]), array([[-32.09288066]]), array([[-32.09567523]]), array([[-32.09280793]]), array([[-32.09095246]]), array([[-32.10212486]]), array([[121.60397346]]), array([[68.37132221]]), array([[-32.09409589]]), array([[143.73115829]]), array([[-32.09584994]]), array([[-32.09347479]]), array([[-32.0983499]]), array([[68.37118549]]), array([[68.3724213]]), array([[-32.10551648]]), array([[-32.08896456]]), array([[-32.09526928]]), array([[-32.09097195]]), array([[121.69105259]]), array([[-32.08351329]]), array([[121.60004924]]), array([[68.37288502]]), array([[122.72019578]]), array([[-32.08143277]]), array([[-32.09552253]]), array([[143.72985918]]), array([[-32.09596859]]), array([[-32.09285021]]), array([[-32.08200266]]), array([[121.60547098]]), array([[-32.09877015]]), array([[-32.09831229]]), array([[-32.07975079]]), array([[-32.09857514]]), array([[-32.09877766]]), array([[-32.09911756]]), array([[-32.08372603]]), array([[68.3714596]]), array([[49.61608038]]), array([[68.37404088]]), array([[143.36182343]]), array([[68.37529814]]), array([[57.90175474]]), array([[-32.10155904]]), array([[-32.09400741]]), array([[-32.09634364]]), array([[-32.09649016]]), array([[-32.08204535]]), array([[68.374033]]), array([[-32.08834719]]), array([[68.37476211]]), array([[-32.09038257]]), array([[-32.09804617]]), array([[120.32585385]]), array([[-32.09338356]]), array([[68.37381511]]), array([[-32.09684434]]), array([[-32.0945042]]), array([[68.36578179]]), array([[122.64830835]]), array([[-32.08799437]]), array([[68.37707814]]), array([[-32.08994749]]), array([[-32.09264054]]), array([[143.72900349]]), array([[-32.09976644]]), array([[-28.79187987]]), array([[-32.09894274]]), array([[-32.10406999]]), array([[-32.09567028]]), array([[-32.10480459]]), array([[68.3783907]]), array([[-32.08766362]]), array([[-32.08943818]]), array([[-32.09855888]]), array([[-32.0943759]]), array([[-32.08769026]]), array([[-32.08329765]]), array([[-32.09800035]]), array([[-32.09197166]]), array([[-32.09403283]]), array([[-32.09733337]]), array([[-32.09449777]]), array([[-32.09654676]]), array([[-32.08555063]])]\n",
            "Actual: [-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
            " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
            "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
            "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
            "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
            "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
            " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
            "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
            " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
            "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
            "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
            "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
            "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
            "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
            "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
            "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
            " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
            "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
            " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
            "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
            "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
            "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
            " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
            " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
            "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n",
            "Average MSE on test data 84.84873321007102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Least MSE on training data is 804.5641082152216 at index 1\n",
            "Average MSE on training data in the new prediction 0.024971254779846498\n",
            "Initial Prediction: [-38.16122867 105.31676633 134.72468027 -38.01796634 -38.10940545\n",
            "  58.63715422  93.39191211 -38.41780508  58.51022608 -38.37872047\n",
            " -38.25643138  92.83437904 102.30223538 -38.33021912 -38.17550963\n",
            " -37.68304183  59.27904909 -37.98174673 -38.05985028 -38.34001661\n",
            "  93.31644449  59.22901894 -37.85016873 135.41454181 -37.50840111\n",
            " -37.50679766 -37.54409317  59.56018496  59.82251868 -37.50894588\n",
            " -36.99732514 -37.42464203 -37.42391051 136.07166841 -37.74195418\n",
            " 112.65552892  72.96945608 134.6750943  -38.15172593 -38.60575698\n",
            " 134.46334339 -38.74227428 -38.60967551 -38.52347265 111.88612206\n",
            " -39.22669657 -38.91020382 -38.59936121 -39.16414784 -35.25990844\n",
            " -38.54534563 -38.28196726  58.49074972  58.76037557  58.53210077\n",
            " 134.04890911  58.3461912   58.00335539 -38.75834808 -38.31648044\n",
            " -38.34845308 -38.10830745 -38.01544372  58.52290064 -38.1592624\n",
            "  69.62562761 -38.28405235 -38.35519075  93.32407327 -37.85872137\n",
            "  93.80712576 -38.04886946 -37.81691516  59.11211189 136.41810306\n",
            " -37.6115783   59.60385776 -38.18227809 -38.37557617 135.25916535\n",
            " -38.3131303   59.8849367  -38.00016497 -37.67268606 -34.82459495\n",
            "  60.72799897  95.26971713 -37.10538799 -37.43779621 -37.52361534\n",
            "  30.48290745 -37.33849229 -37.78787526 -38.19732727 -38.024807\n",
            " -37.89250107 -38.12672238 -37.80587136 -38.10416681 -38.00655781]\n",
            "New Prediction: [array([[-32.09266106]]), array([[121.60295793]]), array([[136.8458323]]), array([[-32.09403208]]), array([[-32.08406455]]), array([[68.37047457]]), array([[68.37327581]]), array([[-32.09433439]]), array([[68.37428306]]), array([[-32.09333155]]), array([[-32.09050937]]), array([[68.37535431]]), array([[143.73087531]]), array([[-32.09052273]]), array([[-32.10545436]]), array([[-32.09288066]]), array([[-32.09567523]]), array([[-32.09280793]]), array([[-32.09095246]]), array([[-32.10212486]]), array([[121.60397346]]), array([[68.37132221]]), array([[-32.09409589]]), array([[143.73115829]]), array([[-32.09584994]]), array([[-32.09347479]]), array([[-32.0983499]]), array([[68.37118549]]), array([[68.3724213]]), array([[-32.10551648]]), array([[-32.08896456]]), array([[-32.09526928]]), array([[-32.09097195]]), array([[121.69105259]]), array([[-32.08351329]]), array([[121.60004924]]), array([[68.37288502]]), array([[122.72019578]]), array([[-32.08143277]]), array([[-32.09552253]]), array([[143.72985918]]), array([[-32.09596859]]), array([[-32.09285021]]), array([[-32.08200266]]), array([[121.60547098]]), array([[-32.09877015]]), array([[-32.09831229]]), array([[-32.07975079]]), array([[-32.09857514]]), array([[-32.09877766]]), array([[-32.09911756]]), array([[-32.08372603]]), array([[68.3714596]]), array([[49.61608038]]), array([[68.37404088]]), array([[143.36182343]]), array([[68.37529814]]), array([[57.90175474]]), array([[-32.10155904]]), array([[-32.09400741]]), array([[-32.09634364]]), array([[-32.09649016]]), array([[-32.08204535]]), array([[68.374033]]), array([[-32.08834719]]), array([[68.37476211]]), array([[-32.09038257]]), array([[-32.09804617]]), array([[120.32585385]]), array([[-32.09338356]]), array([[68.37381511]]), array([[-32.09684434]]), array([[-32.0945042]]), array([[68.36578179]]), array([[122.64830835]]), array([[-32.08799437]]), array([[68.37707814]]), array([[-32.08994749]]), array([[-32.09264054]]), array([[143.72900349]]), array([[-32.09976644]]), array([[-28.79187987]]), array([[-32.09894274]]), array([[-32.10406999]]), array([[-32.09567028]]), array([[-32.10480459]]), array([[68.3783907]]), array([[-32.08766362]]), array([[-32.08943818]]), array([[-32.09855888]]), array([[-32.0943759]]), array([[-32.08769026]]), array([[-32.08329765]]), array([[-32.09800035]]), array([[-32.09197166]]), array([[-32.09403283]]), array([[-32.09733337]]), array([[-32.09449777]]), array([[-32.09654676]]), array([[-32.08555063]]), array([[-38.14238119]]), array([[93.15468121]]), array([[93.15436917]]), array([[-38.14332247]]), array([[-38.1412779]]), array([[58.6378198]]), array([[60.94782014]]), array([[-38.14268553]]), array([[58.63913231]]), array([[-38.14167766]]), array([[-38.14267953]]), array([[72.83186243]]), array([[111.8368088]]), array([[-38.14024962]]), array([[-38.14415661]]), array([[-38.14329388]]), array([[58.63899896]]), array([[-38.1428435]]), array([[-38.14349432]]), array([[-38.14303368]]), array([[93.15511806]]), array([[58.63900548]]), array([[-38.14118097]]), array([[112.40492309]]), array([[-38.14333444]]), array([[-38.14278443]]), array([[-38.14425774]]), array([[58.63810791]]), array([[58.63848499]]), array([[-38.14406753]]), array([[-38.14374278]]), array([[-38.14314738]]), array([[-38.14311791]]), array([[93.15528194]]), array([[-38.14221396]]), array([[93.15507806]]), array([[64.26205047]]), array([[93.15416903]]), array([[-38.14195567]]), array([[-38.14433741]]), array([[96.44927787]]), array([[-38.1426255]]), array([[-38.14177791]]), array([[-38.14337258]]), array([[93.15567223]]), array([[-38.14252004]]), array([[-38.14236704]]), array([[-38.14230039]]), array([[-38.14289249]]), array([[-32.1528652]]), array([[-38.14297873]]), array([[-38.14170815]]), array([[58.63821449]]), array([[58.63859497]]), array([[58.63878423]]), array([[93.16566062]]), array([[58.63938176]]), array([[58.63823999]]), array([[-38.14285744]]), array([[-38.14296747]]), array([[-38.14201481]]), array([[-38.14304823]]), array([[-38.13998462]]), array([[58.64641951]]), array([[-38.14155364]]), array([[76.55103598]]), array([[-38.14201069]]), array([[-38.1419458]]), array([[93.15391838]]), array([[-38.14334331]]), array([[93.15385096]]), array([[-38.14226186]]), array([[-38.14331721]]), array([[58.63664203]]), array([[93.15441905]]), array([[-38.14374183]]), array([[58.63969955]]), array([[-38.14268701]]), array([[-38.14285735]]), array([[112.3275444]]), array([[-38.1423391]]), array([[58.63857084]]), array([[-38.14246227]]), array([[-38.14344754]]), array([[-34.60271306]]), array([[58.63728793]]), array([[77.27915885]]), array([[-38.14286375]]), array([[-38.14175341]]), array([[-38.14242221]]), array([[20.61752536]]), array([[-38.14356419]]), array([[-38.14216088]]), array([[-38.1431001]]), array([[-38.14103317]]), array([[-38.14408485]]), array([[-38.14182761]]), array([[-38.1445024]]), array([[-38.14303377]]), array([[-38.14080549]])]\n",
            "Actual: [-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
            " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
            "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
            "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
            "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
            "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
            " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
            "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
            " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
            "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
            "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
            "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
            "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
            "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
            "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
            "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
            " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
            "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
            " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
            "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
            "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
            "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
            " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
            " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
            "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n",
            "Average MSE on test data 90.4006539698704\n",
            "Least MSE on training data is 605.0922948917084 at index 2\n",
            "Average MSE on training data in the new prediction 0.0004390986902894386\n",
            "Initial Prediction: [-63.66481638  82.42834275  86.30546132 -62.22181439 -62.04954845\n",
            "  92.06989012  90.04926617 -62.81210405  88.48581496 -55.2437727\n",
            " -61.75894391  86.1774867   83.82367816  22.30030029 -16.37305932\n",
            " -61.17833232  88.62177811 -61.75359595 -61.53736569  16.30457694\n",
            "  84.7273626   87.39158599  16.90001188  82.05784205 -61.37145813\n",
            " -60.89026149 -60.54383813  89.95702863  88.16663682   0.94216447\n",
            " -60.18984839 -61.15517203 -60.67924761  88.04794966 -60.63163503\n",
            "  90.71275683  92.35546292  90.50702405 -61.075208   -62.12748339\n",
            "  94.7130349  -61.82963828 -60.92707141 -60.19192713  99.72623367\n",
            "  30.40634449  29.06861592 -62.22686494 -63.63825452  27.72048087\n",
            " -62.33037003 -60.98377828  97.67508675  95.03988229  90.32268623\n",
            "  86.71364601  90.98686229  86.09316901  16.37764236 -62.88567092\n",
            "   1.17274661 -62.49581135 -61.70753715  85.1777137  -62.0464992\n",
            "  83.26906902 -62.0881179   15.69859699  82.06432458 -61.44552873\n",
            "  85.65412947 -15.56538112 -61.26762623  85.93913633  87.97037078\n",
            " -60.35423785  92.10796803 -61.96240938 -62.13267102  87.1392387\n",
            "  25.95554205  94.66324159  22.21832607  21.796247    23.93661374\n",
            "  90.60532634  88.02613639 -62.19628319 -62.80946283  17.90794587\n",
            "  17.69375182 -62.70208449 -63.70176886 -64.55063448  -0.11295453\n",
            " -63.60602823  15.40409175 -63.8765283  -64.3569388  -63.51254419]\n",
            "New Prediction: [array([[-32.09266106]]), array([[121.60295793]]), array([[136.8458323]]), array([[-32.09403208]]), array([[-32.08406455]]), array([[68.37047457]]), array([[68.37327581]]), array([[-32.09433439]]), array([[68.37428306]]), array([[-32.09333155]]), array([[-32.09050937]]), array([[68.37535431]]), array([[143.73087531]]), array([[-32.09052273]]), array([[-32.10545436]]), array([[-32.09288066]]), array([[-32.09567523]]), array([[-32.09280793]]), array([[-32.09095246]]), array([[-32.10212486]]), array([[121.60397346]]), array([[68.37132221]]), array([[-32.09409589]]), array([[143.73115829]]), array([[-32.09584994]]), array([[-32.09347479]]), array([[-32.0983499]]), array([[68.37118549]]), array([[68.3724213]]), array([[-32.10551648]]), array([[-32.08896456]]), array([[-32.09526928]]), array([[-32.09097195]]), array([[121.69105259]]), array([[-32.08351329]]), array([[121.60004924]]), array([[68.37288502]]), array([[122.72019578]]), array([[-32.08143277]]), array([[-32.09552253]]), array([[143.72985918]]), array([[-32.09596859]]), array([[-32.09285021]]), array([[-32.08200266]]), array([[121.60547098]]), array([[-32.09877015]]), array([[-32.09831229]]), array([[-32.07975079]]), array([[-32.09857514]]), array([[-32.09877766]]), array([[-32.09911756]]), array([[-32.08372603]]), array([[68.3714596]]), array([[49.61608038]]), array([[68.37404088]]), array([[143.36182343]]), array([[68.37529814]]), array([[57.90175474]]), array([[-32.10155904]]), array([[-32.09400741]]), array([[-32.09634364]]), array([[-32.09649016]]), array([[-32.08204535]]), array([[68.374033]]), array([[-32.08834719]]), array([[68.37476211]]), array([[-32.09038257]]), array([[-32.09804617]]), array([[120.32585385]]), array([[-32.09338356]]), array([[68.37381511]]), array([[-32.09684434]]), array([[-32.0945042]]), array([[68.36578179]]), array([[122.64830835]]), array([[-32.08799437]]), array([[68.37707814]]), array([[-32.08994749]]), array([[-32.09264054]]), array([[143.72900349]]), array([[-32.09976644]]), array([[-28.79187987]]), array([[-32.09894274]]), array([[-32.10406999]]), array([[-32.09567028]]), array([[-32.10480459]]), array([[68.3783907]]), array([[-32.08766362]]), array([[-32.08943818]]), array([[-32.09855888]]), array([[-32.0943759]]), array([[-32.08769026]]), array([[-32.08329765]]), array([[-32.09800035]]), array([[-32.09197166]]), array([[-32.09403283]]), array([[-32.09733337]]), array([[-32.09449777]]), array([[-32.09654676]]), array([[-32.08555063]]), array([[-38.14238119]]), array([[93.15468121]]), array([[93.15436917]]), array([[-38.14332247]]), array([[-38.1412779]]), array([[58.6378198]]), array([[60.94782014]]), array([[-38.14268553]]), array([[58.63913231]]), array([[-38.14167766]]), array([[-38.14267953]]), array([[72.83186243]]), array([[111.8368088]]), array([[-38.14024962]]), array([[-38.14415661]]), array([[-38.14329388]]), array([[58.63899896]]), array([[-38.1428435]]), array([[-38.14349432]]), array([[-38.14303368]]), array([[93.15511806]]), array([[58.63900548]]), array([[-38.14118097]]), array([[112.40492309]]), array([[-38.14333444]]), array([[-38.14278443]]), array([[-38.14425774]]), array([[58.63810791]]), array([[58.63848499]]), array([[-38.14406753]]), array([[-38.14374278]]), array([[-38.14314738]]), array([[-38.14311791]]), array([[93.15528194]]), array([[-38.14221396]]), array([[93.15507806]]), array([[64.26205047]]), array([[93.15416903]]), array([[-38.14195567]]), array([[-38.14433741]]), array([[96.44927787]]), array([[-38.1426255]]), array([[-38.14177791]]), array([[-38.14337258]]), array([[93.15567223]]), array([[-38.14252004]]), array([[-38.14236704]]), array([[-38.14230039]]), array([[-38.14289249]]), array([[-32.1528652]]), array([[-38.14297873]]), array([[-38.14170815]]), array([[58.63821449]]), array([[58.63859497]]), array([[58.63878423]]), array([[93.16566062]]), array([[58.63938176]]), array([[58.63823999]]), array([[-38.14285744]]), array([[-38.14296747]]), array([[-38.14201481]]), array([[-38.14304823]]), array([[-38.13998462]]), array([[58.64641951]]), array([[-38.14155364]]), array([[76.55103598]]), array([[-38.14201069]]), array([[-38.1419458]]), array([[93.15391838]]), array([[-38.14334331]]), array([[93.15385096]]), array([[-38.14226186]]), array([[-38.14331721]]), array([[58.63664203]]), array([[93.15441905]]), array([[-38.14374183]]), array([[58.63969955]]), array([[-38.14268701]]), array([[-38.14285735]]), array([[112.3275444]]), array([[-38.1423391]]), array([[58.63857084]]), array([[-38.14246227]]), array([[-38.14344754]]), array([[-34.60271306]]), array([[58.63728793]]), array([[77.27915885]]), array([[-38.14286375]]), array([[-38.14175341]]), array([[-38.14242221]]), array([[20.61752536]]), array([[-38.14356419]]), array([[-38.14216088]]), array([[-38.1431001]]), array([[-38.14103317]]), array([[-38.14408485]]), array([[-38.14182761]]), array([[-38.1445024]]), array([[-38.14303377]]), array([[-38.14080549]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]])]\n",
            "Actual: [-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
            " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
            "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
            "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
            "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
            "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
            " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
            "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
            " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
            "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
            "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
            "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
            "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
            "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
            "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
            "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
            " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
            "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
            " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
            "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
            "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
            "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
            " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
            " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
            "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n",
            "Average MSE on test data 122.12376852654818\n",
            "Least MSE on training data is 605.0922948917084 at index 2\n",
            "Average MSE on training data in the new prediction 0.0004390986902894386\n",
            "Initial Prediction: [-62.53004516  37.44948312  47.49948857 -51.74129192 -52.18942599\n",
            "  58.28114481  59.9110455  -58.29766375  62.04682188 -56.75582025\n",
            " -50.22015261  62.4775057   62.49693318  72.66249188  62.5266581\n",
            " -54.4964451   60.04767217 -57.27372502 -55.75413099  56.29340572\n",
            "  53.95997619  60.84913063  59.26507969  52.14208763 -55.30100174\n",
            " -51.5232328  -55.26924851  65.66446977  66.43110665  65.41280501\n",
            " -52.72674671 -66.10177471 -59.20349712  62.04944804 -54.73784026\n",
            "  67.37424851  65.87127223  80.26346768 -64.66954601 -66.14012896\n",
            "  81.83959664 -64.42729812 -62.99555576 -61.83074706 102.83552733\n",
            "  86.14928879  77.45354262 -51.77373396 -61.54942145  69.17321183\n",
            " -57.10111296 -61.16757469  65.0541313   65.8064006   63.65378313\n",
            "  62.52320323  70.55391591  67.13343754  65.82376061 -69.44176654\n",
            "  62.90970814 -65.11612946 -55.02796267  56.44806205 -58.06137786\n",
            "  57.60080809 -57.22146419  58.85878584  54.23627283 -54.77318679\n",
            "  61.71028477  64.38678463 -57.25058981  57.00441242  62.91406062\n",
            " -51.68592938  69.62649755 -67.91027132 -68.52592407  66.74765219\n",
            "  77.95821407  72.04943339  68.82572644  62.5997198   60.91385304\n",
            "  55.23398499  56.38351801 -67.4849687  -73.5738398   55.66377841\n",
            "  50.66255291 -70.78823201 -80.84376905 -86.91607649  46.09749601\n",
            " -70.22386935  38.50236354 -72.62620105 -76.2912056  -63.29424626]\n",
            "New Prediction: [array([[-32.09266106]]), array([[121.60295793]]), array([[136.8458323]]), array([[-32.09403208]]), array([[-32.08406455]]), array([[68.37047457]]), array([[68.37327581]]), array([[-32.09433439]]), array([[68.37428306]]), array([[-32.09333155]]), array([[-32.09050937]]), array([[68.37535431]]), array([[143.73087531]]), array([[-32.09052273]]), array([[-32.10545436]]), array([[-32.09288066]]), array([[-32.09567523]]), array([[-32.09280793]]), array([[-32.09095246]]), array([[-32.10212486]]), array([[121.60397346]]), array([[68.37132221]]), array([[-32.09409589]]), array([[143.73115829]]), array([[-32.09584994]]), array([[-32.09347479]]), array([[-32.0983499]]), array([[68.37118549]]), array([[68.3724213]]), array([[-32.10551648]]), array([[-32.08896456]]), array([[-32.09526928]]), array([[-32.09097195]]), array([[121.69105259]]), array([[-32.08351329]]), array([[121.60004924]]), array([[68.37288502]]), array([[122.72019578]]), array([[-32.08143277]]), array([[-32.09552253]]), array([[143.72985918]]), array([[-32.09596859]]), array([[-32.09285021]]), array([[-32.08200266]]), array([[121.60547098]]), array([[-32.09877015]]), array([[-32.09831229]]), array([[-32.07975079]]), array([[-32.09857514]]), array([[-32.09877766]]), array([[-32.09911756]]), array([[-32.08372603]]), array([[68.3714596]]), array([[49.61608038]]), array([[68.37404088]]), array([[143.36182343]]), array([[68.37529814]]), array([[57.90175474]]), array([[-32.10155904]]), array([[-32.09400741]]), array([[-32.09634364]]), array([[-32.09649016]]), array([[-32.08204535]]), array([[68.374033]]), array([[-32.08834719]]), array([[68.37476211]]), array([[-32.09038257]]), array([[-32.09804617]]), array([[120.32585385]]), array([[-32.09338356]]), array([[68.37381511]]), array([[-32.09684434]]), array([[-32.0945042]]), array([[68.36578179]]), array([[122.64830835]]), array([[-32.08799437]]), array([[68.37707814]]), array([[-32.08994749]]), array([[-32.09264054]]), array([[143.72900349]]), array([[-32.09976644]]), array([[-28.79187987]]), array([[-32.09894274]]), array([[-32.10406999]]), array([[-32.09567028]]), array([[-32.10480459]]), array([[68.3783907]]), array([[-32.08766362]]), array([[-32.08943818]]), array([[-32.09855888]]), array([[-32.0943759]]), array([[-32.08769026]]), array([[-32.08329765]]), array([[-32.09800035]]), array([[-32.09197166]]), array([[-32.09403283]]), array([[-32.09733337]]), array([[-32.09449777]]), array([[-32.09654676]]), array([[-32.08555063]]), array([[-38.14238119]]), array([[93.15468121]]), array([[93.15436917]]), array([[-38.14332247]]), array([[-38.1412779]]), array([[58.6378198]]), array([[60.94782014]]), array([[-38.14268553]]), array([[58.63913231]]), array([[-38.14167766]]), array([[-38.14267953]]), array([[72.83186243]]), array([[111.8368088]]), array([[-38.14024962]]), array([[-38.14415661]]), array([[-38.14329388]]), array([[58.63899896]]), array([[-38.1428435]]), array([[-38.14349432]]), array([[-38.14303368]]), array([[93.15511806]]), array([[58.63900548]]), array([[-38.14118097]]), array([[112.40492309]]), array([[-38.14333444]]), array([[-38.14278443]]), array([[-38.14425774]]), array([[58.63810791]]), array([[58.63848499]]), array([[-38.14406753]]), array([[-38.14374278]]), array([[-38.14314738]]), array([[-38.14311791]]), array([[93.15528194]]), array([[-38.14221396]]), array([[93.15507806]]), array([[64.26205047]]), array([[93.15416903]]), array([[-38.14195567]]), array([[-38.14433741]]), array([[96.44927787]]), array([[-38.1426255]]), array([[-38.14177791]]), array([[-38.14337258]]), array([[93.15567223]]), array([[-38.14252004]]), array([[-38.14236704]]), array([[-38.14230039]]), array([[-38.14289249]]), array([[-32.1528652]]), array([[-38.14297873]]), array([[-38.14170815]]), array([[58.63821449]]), array([[58.63859497]]), array([[58.63878423]]), array([[93.16566062]]), array([[58.63938176]]), array([[58.63823999]]), array([[-38.14285744]]), array([[-38.14296747]]), array([[-38.14201481]]), array([[-38.14304823]]), array([[-38.13998462]]), array([[58.64641951]]), array([[-38.14155364]]), array([[76.55103598]]), array([[-38.14201069]]), array([[-38.1419458]]), array([[93.15391838]]), array([[-38.14334331]]), array([[93.15385096]]), array([[-38.14226186]]), array([[-38.14331721]]), array([[58.63664203]]), array([[93.15441905]]), array([[-38.14374183]]), array([[58.63969955]]), array([[-38.14268701]]), array([[-38.14285735]]), array([[112.3275444]]), array([[-38.1423391]]), array([[58.63857084]]), array([[-38.14246227]]), array([[-38.14344754]]), array([[-34.60271306]]), array([[58.63728793]]), array([[77.27915885]]), array([[-38.14286375]]), array([[-38.14175341]]), array([[-38.14242221]]), array([[20.61752536]]), array([[-38.14356419]]), array([[-38.14216088]]), array([[-38.1431001]]), array([[-38.14103317]]), array([[-38.14408485]]), array([[-38.14182761]]), array([[-38.1445024]]), array([[-38.14303377]]), array([[-38.14080549]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]])]\n",
            "Actual: [-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
            " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
            "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
            "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
            "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
            "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
            " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
            "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
            " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
            "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
            "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
            "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
            "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
            "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
            "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
            "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
            " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
            "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
            " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
            "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
            "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
            "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
            " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
            " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
            "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n",
            "Average MSE on test data 121.79705759819788\n",
            "Least MSE on training data is 605.0922948917084 at index 2\n",
            "Average MSE on training data in the new prediction 0.0004390986902894386\n",
            "Initial Prediction: [-73.3697719   29.04287879 130.41519348 -59.38532073 -58.80764038\n",
            " 144.79656233  64.57035944 -67.60885795  75.58399966   7.02456367\n",
            " -68.23623967  53.47918308  63.55542314 155.57537229 -12.58840939\n",
            " -69.79073171 -16.53862429 -63.99661044 -62.69115772  38.66417503\n",
            "  36.66655813 118.83990034  40.52958669  -4.15546488 -52.88906201\n",
            " -51.33131839 -50.55509592 174.17008923  61.84141257  57.55564436\n",
            " -61.29738191 -66.01324672 -63.14781719  43.90302883 -56.68934206\n",
            " 121.83441043 105.67035849  70.69114577 -59.44817658 -64.87225668\n",
            " 139.49688085  19.29962227  12.09924216 -72.28233783 123.99632008\n",
            " 100.84259509   4.96895163 -89.79472312 -26.99968801  15.11242698\n",
            " -18.81641652 -87.60582477  32.6406595   75.19128042  45.13558581\n",
            "  55.1419538  136.87619435  32.56730305  58.58467914 -88.09295053\n",
            "  37.21402698 -87.18592301 -70.79619424   5.11272083 -74.5412858\n",
            "  73.37464727 -73.3580803   67.9314217   15.57848636 -65.573486\n",
            " 128.5985301    6.96894296 -70.07981618  80.42018689 108.83461017\n",
            " -62.80416072 132.19772435 -77.09770527 -76.43548386  36.37810281\n",
            " 172.54809105  14.33540344  48.27018938  13.80955263  52.02495787\n",
            "   9.73728543  70.32104099 -72.88780506 -74.88455105  49.67338645\n",
            "  10.4182506  -76.16434241 -79.81653846 -12.84985748   4.42928898\n",
            " -84.11484908 -16.70965004 -80.17639058 -80.94843756 -75.06353863]\n",
            "New Prediction: [array([[-32.09266106]]), array([[121.60295793]]), array([[136.8458323]]), array([[-32.09403208]]), array([[-32.08406455]]), array([[68.37047457]]), array([[68.37327581]]), array([[-32.09433439]]), array([[68.37428306]]), array([[-32.09333155]]), array([[-32.09050937]]), array([[68.37535431]]), array([[143.73087531]]), array([[-32.09052273]]), array([[-32.10545436]]), array([[-32.09288066]]), array([[-32.09567523]]), array([[-32.09280793]]), array([[-32.09095246]]), array([[-32.10212486]]), array([[121.60397346]]), array([[68.37132221]]), array([[-32.09409589]]), array([[143.73115829]]), array([[-32.09584994]]), array([[-32.09347479]]), array([[-32.0983499]]), array([[68.37118549]]), array([[68.3724213]]), array([[-32.10551648]]), array([[-32.08896456]]), array([[-32.09526928]]), array([[-32.09097195]]), array([[121.69105259]]), array([[-32.08351329]]), array([[121.60004924]]), array([[68.37288502]]), array([[122.72019578]]), array([[-32.08143277]]), array([[-32.09552253]]), array([[143.72985918]]), array([[-32.09596859]]), array([[-32.09285021]]), array([[-32.08200266]]), array([[121.60547098]]), array([[-32.09877015]]), array([[-32.09831229]]), array([[-32.07975079]]), array([[-32.09857514]]), array([[-32.09877766]]), array([[-32.09911756]]), array([[-32.08372603]]), array([[68.3714596]]), array([[49.61608038]]), array([[68.37404088]]), array([[143.36182343]]), array([[68.37529814]]), array([[57.90175474]]), array([[-32.10155904]]), array([[-32.09400741]]), array([[-32.09634364]]), array([[-32.09649016]]), array([[-32.08204535]]), array([[68.374033]]), array([[-32.08834719]]), array([[68.37476211]]), array([[-32.09038257]]), array([[-32.09804617]]), array([[120.32585385]]), array([[-32.09338356]]), array([[68.37381511]]), array([[-32.09684434]]), array([[-32.0945042]]), array([[68.36578179]]), array([[122.64830835]]), array([[-32.08799437]]), array([[68.37707814]]), array([[-32.08994749]]), array([[-32.09264054]]), array([[143.72900349]]), array([[-32.09976644]]), array([[-28.79187987]]), array([[-32.09894274]]), array([[-32.10406999]]), array([[-32.09567028]]), array([[-32.10480459]]), array([[68.3783907]]), array([[-32.08766362]]), array([[-32.08943818]]), array([[-32.09855888]]), array([[-32.0943759]]), array([[-32.08769026]]), array([[-32.08329765]]), array([[-32.09800035]]), array([[-32.09197166]]), array([[-32.09403283]]), array([[-32.09733337]]), array([[-32.09449777]]), array([[-32.09654676]]), array([[-32.08555063]]), array([[-38.14238119]]), array([[93.15468121]]), array([[93.15436917]]), array([[-38.14332247]]), array([[-38.1412779]]), array([[58.6378198]]), array([[60.94782014]]), array([[-38.14268553]]), array([[58.63913231]]), array([[-38.14167766]]), array([[-38.14267953]]), array([[72.83186243]]), array([[111.8368088]]), array([[-38.14024962]]), array([[-38.14415661]]), array([[-38.14329388]]), array([[58.63899896]]), array([[-38.1428435]]), array([[-38.14349432]]), array([[-38.14303368]]), array([[93.15511806]]), array([[58.63900548]]), array([[-38.14118097]]), array([[112.40492309]]), array([[-38.14333444]]), array([[-38.14278443]]), array([[-38.14425774]]), array([[58.63810791]]), array([[58.63848499]]), array([[-38.14406753]]), array([[-38.14374278]]), array([[-38.14314738]]), array([[-38.14311791]]), array([[93.15528194]]), array([[-38.14221396]]), array([[93.15507806]]), array([[64.26205047]]), array([[93.15416903]]), array([[-38.14195567]]), array([[-38.14433741]]), array([[96.44927787]]), array([[-38.1426255]]), array([[-38.14177791]]), array([[-38.14337258]]), array([[93.15567223]]), array([[-38.14252004]]), array([[-38.14236704]]), array([[-38.14230039]]), array([[-38.14289249]]), array([[-32.1528652]]), array([[-38.14297873]]), array([[-38.14170815]]), array([[58.63821449]]), array([[58.63859497]]), array([[58.63878423]]), array([[93.16566062]]), array([[58.63938176]]), array([[58.63823999]]), array([[-38.14285744]]), array([[-38.14296747]]), array([[-38.14201481]]), array([[-38.14304823]]), array([[-38.13998462]]), array([[58.64641951]]), array([[-38.14155364]]), array([[76.55103598]]), array([[-38.14201069]]), array([[-38.1419458]]), array([[93.15391838]]), array([[-38.14334331]]), array([[93.15385096]]), array([[-38.14226186]]), array([[-38.14331721]]), array([[58.63664203]]), array([[93.15441905]]), array([[-38.14374183]]), array([[58.63969955]]), array([[-38.14268701]]), array([[-38.14285735]]), array([[112.3275444]]), array([[-38.1423391]]), array([[58.63857084]]), array([[-38.14246227]]), array([[-38.14344754]]), array([[-34.60271306]]), array([[58.63728793]]), array([[77.27915885]]), array([[-38.14286375]]), array([[-38.14175341]]), array([[-38.14242221]]), array([[20.61752536]]), array([[-38.14356419]]), array([[-38.14216088]]), array([[-38.1431001]]), array([[-38.14103317]]), array([[-38.14408485]]), array([[-38.14182761]]), array([[-38.1445024]]), array([[-38.14303377]]), array([[-38.14080549]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]])]\n",
            "Actual: [-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
            " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
            "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
            "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
            "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
            "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
            " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
            "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
            " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
            "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
            "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
            "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
            "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
            "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
            "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
            "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
            " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
            "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
            " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
            "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
            "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
            "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
            " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
            " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
            "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n",
            "Average MSE on test data 140.77078758403613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: RuntimeWarning: overflow encountered in multiply\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in multiply\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: RuntimeWarning: overflow encountered in square\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Least MSE on training data is 605.0922948917084 at index 2\n",
            "Average MSE on training data in the new prediction 0.0004390986902894386\n",
            "Initial Prediction: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan]\n",
            "New Prediction: [array([[-32.09266106]]), array([[121.60295793]]), array([[136.8458323]]), array([[-32.09403208]]), array([[-32.08406455]]), array([[68.37047457]]), array([[68.37327581]]), array([[-32.09433439]]), array([[68.37428306]]), array([[-32.09333155]]), array([[-32.09050937]]), array([[68.37535431]]), array([[143.73087531]]), array([[-32.09052273]]), array([[-32.10545436]]), array([[-32.09288066]]), array([[-32.09567523]]), array([[-32.09280793]]), array([[-32.09095246]]), array([[-32.10212486]]), array([[121.60397346]]), array([[68.37132221]]), array([[-32.09409589]]), array([[143.73115829]]), array([[-32.09584994]]), array([[-32.09347479]]), array([[-32.0983499]]), array([[68.37118549]]), array([[68.3724213]]), array([[-32.10551648]]), array([[-32.08896456]]), array([[-32.09526928]]), array([[-32.09097195]]), array([[121.69105259]]), array([[-32.08351329]]), array([[121.60004924]]), array([[68.37288502]]), array([[122.72019578]]), array([[-32.08143277]]), array([[-32.09552253]]), array([[143.72985918]]), array([[-32.09596859]]), array([[-32.09285021]]), array([[-32.08200266]]), array([[121.60547098]]), array([[-32.09877015]]), array([[-32.09831229]]), array([[-32.07975079]]), array([[-32.09857514]]), array([[-32.09877766]]), array([[-32.09911756]]), array([[-32.08372603]]), array([[68.3714596]]), array([[49.61608038]]), array([[68.37404088]]), array([[143.36182343]]), array([[68.37529814]]), array([[57.90175474]]), array([[-32.10155904]]), array([[-32.09400741]]), array([[-32.09634364]]), array([[-32.09649016]]), array([[-32.08204535]]), array([[68.374033]]), array([[-32.08834719]]), array([[68.37476211]]), array([[-32.09038257]]), array([[-32.09804617]]), array([[120.32585385]]), array([[-32.09338356]]), array([[68.37381511]]), array([[-32.09684434]]), array([[-32.0945042]]), array([[68.36578179]]), array([[122.64830835]]), array([[-32.08799437]]), array([[68.37707814]]), array([[-32.08994749]]), array([[-32.09264054]]), array([[143.72900349]]), array([[-32.09976644]]), array([[-28.79187987]]), array([[-32.09894274]]), array([[-32.10406999]]), array([[-32.09567028]]), array([[-32.10480459]]), array([[68.3783907]]), array([[-32.08766362]]), array([[-32.08943818]]), array([[-32.09855888]]), array([[-32.0943759]]), array([[-32.08769026]]), array([[-32.08329765]]), array([[-32.09800035]]), array([[-32.09197166]]), array([[-32.09403283]]), array([[-32.09733337]]), array([[-32.09449777]]), array([[-32.09654676]]), array([[-32.08555063]]), array([[-38.14238119]]), array([[93.15468121]]), array([[93.15436917]]), array([[-38.14332247]]), array([[-38.1412779]]), array([[58.6378198]]), array([[60.94782014]]), array([[-38.14268553]]), array([[58.63913231]]), array([[-38.14167766]]), array([[-38.14267953]]), array([[72.83186243]]), array([[111.8368088]]), array([[-38.14024962]]), array([[-38.14415661]]), array([[-38.14329388]]), array([[58.63899896]]), array([[-38.1428435]]), array([[-38.14349432]]), array([[-38.14303368]]), array([[93.15511806]]), array([[58.63900548]]), array([[-38.14118097]]), array([[112.40492309]]), array([[-38.14333444]]), array([[-38.14278443]]), array([[-38.14425774]]), array([[58.63810791]]), array([[58.63848499]]), array([[-38.14406753]]), array([[-38.14374278]]), array([[-38.14314738]]), array([[-38.14311791]]), array([[93.15528194]]), array([[-38.14221396]]), array([[93.15507806]]), array([[64.26205047]]), array([[93.15416903]]), array([[-38.14195567]]), array([[-38.14433741]]), array([[96.44927787]]), array([[-38.1426255]]), array([[-38.14177791]]), array([[-38.14337258]]), array([[93.15567223]]), array([[-38.14252004]]), array([[-38.14236704]]), array([[-38.14230039]]), array([[-38.14289249]]), array([[-32.1528652]]), array([[-38.14297873]]), array([[-38.14170815]]), array([[58.63821449]]), array([[58.63859497]]), array([[58.63878423]]), array([[93.16566062]]), array([[58.63938176]]), array([[58.63823999]]), array([[-38.14285744]]), array([[-38.14296747]]), array([[-38.14201481]]), array([[-38.14304823]]), array([[-38.13998462]]), array([[58.64641951]]), array([[-38.14155364]]), array([[76.55103598]]), array([[-38.14201069]]), array([[-38.1419458]]), array([[93.15391838]]), array([[-38.14334331]]), array([[93.15385096]]), array([[-38.14226186]]), array([[-38.14331721]]), array([[58.63664203]]), array([[93.15441905]]), array([[-38.14374183]]), array([[58.63969955]]), array([[-38.14268701]]), array([[-38.14285735]]), array([[112.3275444]]), array([[-38.1423391]]), array([[58.63857084]]), array([[-38.14246227]]), array([[-38.14344754]]), array([[-34.60271306]]), array([[58.63728793]]), array([[77.27915885]]), array([[-38.14286375]]), array([[-38.14175341]]), array([[-38.14242221]]), array([[20.61752536]]), array([[-38.14356419]]), array([[-38.14216088]]), array([[-38.1431001]]), array([[-38.14103317]]), array([[-38.14408485]]), array([[-38.14182761]]), array([[-38.1445024]]), array([[-38.14303377]]), array([[-38.14080549]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]])]\n",
            "Actual: [-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
            " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
            "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
            "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
            "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
            "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
            " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
            "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
            " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
            "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
            "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
            "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
            "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
            "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
            "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
            "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
            " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
            "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
            " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
            "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
            "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
            "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
            " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
            " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
            "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n",
            "Average MSE on test data nan\n",
            "Least MSE on training data is 605.0922948917084 at index 2\n",
            "Average MSE on training data in the new prediction 0.0004390986902894386\n",
            "Initial Prediction: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan]\n",
            "New Prediction: [array([[-32.09266106]]), array([[121.60295793]]), array([[136.8458323]]), array([[-32.09403208]]), array([[-32.08406455]]), array([[68.37047457]]), array([[68.37327581]]), array([[-32.09433439]]), array([[68.37428306]]), array([[-32.09333155]]), array([[-32.09050937]]), array([[68.37535431]]), array([[143.73087531]]), array([[-32.09052273]]), array([[-32.10545436]]), array([[-32.09288066]]), array([[-32.09567523]]), array([[-32.09280793]]), array([[-32.09095246]]), array([[-32.10212486]]), array([[121.60397346]]), array([[68.37132221]]), array([[-32.09409589]]), array([[143.73115829]]), array([[-32.09584994]]), array([[-32.09347479]]), array([[-32.0983499]]), array([[68.37118549]]), array([[68.3724213]]), array([[-32.10551648]]), array([[-32.08896456]]), array([[-32.09526928]]), array([[-32.09097195]]), array([[121.69105259]]), array([[-32.08351329]]), array([[121.60004924]]), array([[68.37288502]]), array([[122.72019578]]), array([[-32.08143277]]), array([[-32.09552253]]), array([[143.72985918]]), array([[-32.09596859]]), array([[-32.09285021]]), array([[-32.08200266]]), array([[121.60547098]]), array([[-32.09877015]]), array([[-32.09831229]]), array([[-32.07975079]]), array([[-32.09857514]]), array([[-32.09877766]]), array([[-32.09911756]]), array([[-32.08372603]]), array([[68.3714596]]), array([[49.61608038]]), array([[68.37404088]]), array([[143.36182343]]), array([[68.37529814]]), array([[57.90175474]]), array([[-32.10155904]]), array([[-32.09400741]]), array([[-32.09634364]]), array([[-32.09649016]]), array([[-32.08204535]]), array([[68.374033]]), array([[-32.08834719]]), array([[68.37476211]]), array([[-32.09038257]]), array([[-32.09804617]]), array([[120.32585385]]), array([[-32.09338356]]), array([[68.37381511]]), array([[-32.09684434]]), array([[-32.0945042]]), array([[68.36578179]]), array([[122.64830835]]), array([[-32.08799437]]), array([[68.37707814]]), array([[-32.08994749]]), array([[-32.09264054]]), array([[143.72900349]]), array([[-32.09976644]]), array([[-28.79187987]]), array([[-32.09894274]]), array([[-32.10406999]]), array([[-32.09567028]]), array([[-32.10480459]]), array([[68.3783907]]), array([[-32.08766362]]), array([[-32.08943818]]), array([[-32.09855888]]), array([[-32.0943759]]), array([[-32.08769026]]), array([[-32.08329765]]), array([[-32.09800035]]), array([[-32.09197166]]), array([[-32.09403283]]), array([[-32.09733337]]), array([[-32.09449777]]), array([[-32.09654676]]), array([[-32.08555063]]), array([[-38.14238119]]), array([[93.15468121]]), array([[93.15436917]]), array([[-38.14332247]]), array([[-38.1412779]]), array([[58.6378198]]), array([[60.94782014]]), array([[-38.14268553]]), array([[58.63913231]]), array([[-38.14167766]]), array([[-38.14267953]]), array([[72.83186243]]), array([[111.8368088]]), array([[-38.14024962]]), array([[-38.14415661]]), array([[-38.14329388]]), array([[58.63899896]]), array([[-38.1428435]]), array([[-38.14349432]]), array([[-38.14303368]]), array([[93.15511806]]), array([[58.63900548]]), array([[-38.14118097]]), array([[112.40492309]]), array([[-38.14333444]]), array([[-38.14278443]]), array([[-38.14425774]]), array([[58.63810791]]), array([[58.63848499]]), array([[-38.14406753]]), array([[-38.14374278]]), array([[-38.14314738]]), array([[-38.14311791]]), array([[93.15528194]]), array([[-38.14221396]]), array([[93.15507806]]), array([[64.26205047]]), array([[93.15416903]]), array([[-38.14195567]]), array([[-38.14433741]]), array([[96.44927787]]), array([[-38.1426255]]), array([[-38.14177791]]), array([[-38.14337258]]), array([[93.15567223]]), array([[-38.14252004]]), array([[-38.14236704]]), array([[-38.14230039]]), array([[-38.14289249]]), array([[-32.1528652]]), array([[-38.14297873]]), array([[-38.14170815]]), array([[58.63821449]]), array([[58.63859497]]), array([[58.63878423]]), array([[93.16566062]]), array([[58.63938176]]), array([[58.63823999]]), array([[-38.14285744]]), array([[-38.14296747]]), array([[-38.14201481]]), array([[-38.14304823]]), array([[-38.13998462]]), array([[58.64641951]]), array([[-38.14155364]]), array([[76.55103598]]), array([[-38.14201069]]), array([[-38.1419458]]), array([[93.15391838]]), array([[-38.14334331]]), array([[93.15385096]]), array([[-38.14226186]]), array([[-38.14331721]]), array([[58.63664203]]), array([[93.15441905]]), array([[-38.14374183]]), array([[58.63969955]]), array([[-38.14268701]]), array([[-38.14285735]]), array([[112.3275444]]), array([[-38.1423391]]), array([[58.63857084]]), array([[-38.14246227]]), array([[-38.14344754]]), array([[-34.60271306]]), array([[58.63728793]]), array([[77.27915885]]), array([[-38.14286375]]), array([[-38.14175341]]), array([[-38.14242221]]), array([[20.61752536]]), array([[-38.14356419]]), array([[-38.14216088]]), array([[-38.1431001]]), array([[-38.14103317]]), array([[-38.14408485]]), array([[-38.14182761]]), array([[-38.1445024]]), array([[-38.14303377]]), array([[-38.14080549]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-62.61111035]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-16.83692912]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-1.0395842]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206566]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45164179]]), array([[15.66224932]]), array([[-63.44605066]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[81.90788489]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-1.06849292]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[15.66224708]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-16.83693558]]), array([[-63.45206583]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[81.90788489]]), array([[15.66224708]]), array([[81.90788489]]), array([[15.66224708]]), array([[15.66224708]]), array([[15.66224771]]), array([[81.90788489]]), array([[81.90788489]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[15.66224708]]), array([[16.92222619]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-1.0685259]]), array([[-63.45206583]]), array([[15.66224708]]), array([[-63.45206583]]), array([[-63.45206583]]), array([[-63.45206583]])]\n",
            "Actual: [-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
            " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
            "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
            "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
            "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
            "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
            " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
            "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
            " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
            "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
            "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
            "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
            "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
            "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
            "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
            "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
            " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
            "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
            " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
            "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
            "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
            "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
            " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
            " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
            "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n",
            "Average MSE on test data nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-286-3fdbdd6086d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_test_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_test_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'green'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_test_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (100,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion 2.5 :\n",
        "\n",
        "In this experiment, I used ten neurons in the hidden layer and one neuron in the output layer, and various learning rate=0.001 also used backpropagation 100 times. We see different losses in different learning rates. The result shows that lower learning rates give us better accuracy. "
      ],
      "metadata": {
        "id": "BDzZ3AMbj9pK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pronlem 2.6 "
      ],
      "metadata": {
        "id": "kRrw8qOcly56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import legend\n",
        "##final revised  \n",
        "\n",
        "#creating neural networks for two layers \n",
        "\n",
        "#Setting the hyperparameters \n",
        "N=X_train.shape[0]\n",
        "h_1 = 10 # Number of neurons\n",
        "learning_rate = 0.001\n",
        "num_iters = 100\n",
        "h_o = 1              # output_layer_neurons\n",
        "f = X_train.shape[1] # number of feature in the traing and test data\n",
        "\n",
        "# make the iteration into a list\n",
        "iter_list = [] \n",
        "for i in range(num_iters):\n",
        "  iter_list.append(i)\n",
        "\n",
        "# Randomly initialize weight w1,w2 and bias terms b1, b2\n",
        "w1 = np.random.rand(h_1,f)\n",
        "b1 = np.random.rand(h_1,1)\n",
        "w2 = np.random.rand(h_o,h_1) \n",
        "b2 = np.random.rand(h_o,1)\n",
        "\n",
        "# Define the training set and test set \n",
        "x=X_train\n",
        "y=Y_train\n",
        "\n",
        "# Define the empty set to strore the many updates \n",
        "\n",
        "mse_list = []\n",
        "avg_mse_list = []\n",
        "mse_list_alt = []\n",
        "accuracy_list = []\n",
        "avg_mse_list = []\n",
        "truep_list = []\n",
        "truenp_list = []\n",
        "\n",
        "\n",
        "# Record weight and bias values \n",
        "w1_list = []\n",
        "w2_list = []\n",
        "b1_list= []\n",
        "b2_list = []\n",
        "a2_new_list = []\n",
        "#Working for the generalization for the neural networks\n",
        "f=x.shape[1]\n",
        "\n",
        "# Define the sigmoid function for the hidden layer\n",
        "def sigmoid_fun(z):\n",
        "  g1=1/(1+np.exp(-z))\n",
        "  return g1\n",
        "\n",
        "# Define the activation function for the output layer \n",
        "def linear_fun(z,a,b):\n",
        "  g2=a*z+b\n",
        "  return g2\n",
        "\n",
        "for i in range(x.shape[0]):\n",
        "  y_pred = []\n",
        "  mse_j_collect = []\n",
        "  #avg_mse_list = []\n",
        "  #truep_list = []\n",
        "  #truenp_list = []\n",
        "  #accuracy_list = []\n",
        "  for j in range(x.shape[0]):\n",
        "    z1 = np.dot(w1,x[j].reshape(f,1))+b1\n",
        "    a1 = sigmoid_fun(z1)\n",
        "    z2 = np.dot(w2,a1)+b2\n",
        "    a2 = linear_fun(z2,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "    # want to store this predicted score in the y_pred\n",
        "    y_pred.append(a2)\n",
        "\n",
        "\n",
        "    # calculate the accuaracy\n",
        "    #if y[j]== a2 or np.round(a2,0):\n",
        "     # truep_list.append(1)\n",
        "    #else:\n",
        "     # truenp_list.append(1)\n",
        "       \n",
        "    #y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten() is also useful\n",
        "    #Using Backprop update the weights and bias terms \n",
        "    # Backward pass\n",
        "    dL_dw = (a2-y[j])\n",
        "  \n",
        "    # update weights\n",
        "    w2 = w2-(learning_rate)*dL_dw*a1.T  # check the matrix \n",
        "    w1 = w1-(learning_rate)*dL_dw*np.dot(w2,a1)*np.dot(1-a1,x[1].reshape(2,1).T)\n",
        "    b2 = b2-(learning_rate)*dL_dw  # check the matrix \n",
        "    b1 = b1-(learning_rate)*dL_dw*w2.T*np.dot(a1.T,1-a1)\n",
        "\n",
        "    # mse calculate\n",
        "    mse_j_ind=(a2-y[j])**2\n",
        "    mse_j_collect.append(mse_j_ind)\n",
        "\n",
        "\n",
        "  d=sum(mse_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "  mse_list_alt.append(d)\n",
        "  w1_list.append(w1)\n",
        "  w2_list.append(w2)\n",
        "  b1_list.append(b1)\n",
        "  b2_list.append(b2)\n",
        "\n",
        "  # update the y_pred in each epoch \n",
        "  y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten()\n",
        "  mse_ = np.sum((y_pred-y)**2)/(2*x.shape[0])\n",
        "  mse_list.append(mse_)\n",
        "  #accuary_ind_list=len(truep_list)/100\n",
        "  #accuracy_list.append(accuary_ind_list)\n",
        "\n",
        "\n",
        "# avg mse calculation using the training data\n",
        "avg_mse_list_alt=np.sum(mse_list_alt)/x.shape[0]\n",
        "print('Alternative option: Average MSE on training data',avg_mse_list_alt)\n",
        "avg_mse_list=np.sum(mse_list)/x.shape[0]\n",
        "print('Average MSE on training data',avg_mse_list) # Both ways give us the same average MSE on training data\n",
        "plt.plot(iter_list, mse_list)\n",
        "plt.xlabel('Number of iteration')\n",
        "plt.ylabel('Mean square Loss')\n",
        "plt.title('Number of iteration Vs. Mean square Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# using the test data we will calculate the errorn on the \n",
        "\n",
        "# To do this, we choose the weights that do well on your training set and can generalize well. \n",
        "## This usually is the weights producing the least MSE on training data. Ref: Dr. Sathya\n",
        "#For future reference, this is the default for any and all ML algorithms. \n",
        "## We should always pick the model with the best training accuracy and error. Ref: Dr. Sathya\n",
        "#First, we will see how find the best weights that gives least MSE and best accuracy on training data fir the model predictors\n",
        "# Perhaps, we don't compute the accuracy so we only consider least MSE\n",
        "\n",
        "minimum = mse_list[0]\n",
        "index = 0\n",
        "for i in range(len(mse_list)):\n",
        "  if mse_list[i] < minimum:\n",
        "    minimum = mse_list[i]\n",
        "    index= i\n",
        "print('Least MSE on training data is', minimum,'at index',index) \n",
        "\n",
        "\n",
        "# Fwd Pass with new weights\n",
        "y_pred_train_new = []\n",
        "mse_train_new_list =[]\n",
        "mse_train_new_ind = []\n",
        "a2_train_new_list = []\n",
        "mse_new_j_collect = []\n",
        "mse_new_list_alt = []\n",
        "w1_new=w1_list[index]\n",
        "w2_new=w2_list[index]\n",
        "b1_new=b1_list[index]\n",
        "b2_new=b2_list[index]\n",
        "for j in range (100):\n",
        "  z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "  a1_new = sigmoid_fun(z1_new)\n",
        "  z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "  a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "  a2_new_list.append(a2_new)\n",
        "  y_pred_train_new.append(a2)\n",
        "  \n",
        "# mse calculate\n",
        "  mse_new_j_ind=(a2_new-y[j])**2\n",
        "  mse_new_j_collect.append(mse_new_j_ind)\n",
        "\n",
        "\n",
        "d=sum(mse_new_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "mse_new_list_alt.append(d)\n",
        "\n",
        "# avg mse calculation using the training data\n",
        "avg_mse_new_list_alt=np.sum(mse_new_list_alt)/x.shape[0]\n",
        "print('Average MSE on training data in the new prediction',avg_mse_new_list_alt)\n",
        "#plt.plot(iter_list, mse_new_j_collect)\n",
        "#plt.xlabel('Number of iteration')\n",
        "#plt.ylabel('Mean square Loss')\n",
        "#plt.title('Number of iteration Vs. Mean square Loss')\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Initial Prediction\", y_pred, \"New Prediction:\", a2_new_list, \"Actual:\", y)\n",
        "#print(\"Initial Prediction\",        \"New Prediction:\",       \"Actual:\")\n",
        "#print( y_pred, a2_new_list, y)\n",
        "\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"New Prediction:\",a2_new_list)\n",
        "print(\"Actual:\",y)\n",
        "\n",
        "# alternative way to print not perfect for the moment\n",
        "#for a,b,c in zip(y_pred[::100],a2_new_list[1::100],y[2::100]):\n",
        "  #print '{:<30}{:<30}{:<}'.format(a,b,c)\n",
        "\n",
        "\n",
        "\n",
        "# Analysis on the Test data \n",
        "#X_test\n",
        "#Y_test\n",
        "mse_test_list =[]\n",
        "mse_list_ind = []\n",
        "a2_test_list = []\n",
        "# Fwd Pass with new weights\n",
        "w1_new=w1_list[index]\n",
        "w2_new=w2_list[index]\n",
        "b1_new=b1_list[index]\n",
        "b2_new=b2_list[index]\n",
        "for j in range (X_test.shape[0]):\n",
        "  z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "  a1_new = sigmoid_fun(z1_new)\n",
        "  z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "  a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "  a2_test_list.append(a2_new)\n",
        "  mse_j_test=(a2-y[j])**2\n",
        "  mse_list_ind.append(mse_j_test)\n",
        "\n",
        "dd=sum(mse_list_ind)/(2*X_test.shape[0]) # mean square error in each iteration\n",
        "mse_test_list.append(dd)\n",
        "avg_mse_test_list=np.sum(mse_test_list)/X_test.shape[0]\n",
        "print('Average MSE on test data',avg_mse_test_list) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PMYKPxw_l2FE",
        "outputId": "53bb8401-39d8-421e-974b-a20c6d09848e"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alternative option: Average MSE on training data 1303.7301340309966\n",
            "Average MSE on training data 1303.7301340309964\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Least MSE on training data is 468.287736536344 at index 91\n",
            "Average MSE on training data in the new prediction 7.206709677720289\n",
            "Initial Prediction: [-52.20325394 114.45461128 119.15046355 -52.15878563 -52.16023528\n",
            "  68.19586541  93.14437707 -52.20675615  80.53615708 -52.1958306\n",
            " -52.17440585  81.97181274  92.91914864  15.16300838 -51.48022331\n",
            " -52.08443336  25.40499741 -52.08612482 -52.08500046  17.43802459\n",
            " 117.19609218  57.90623425  15.26955418  99.15445892 -52.07395719\n",
            " -52.05918416 -52.05074978  85.12105975  49.94405522 -10.46320236\n",
            " -51.9877146  -52.04417016 -52.02949768 119.11107566 -52.05123931\n",
            " 119.00620854  99.11596947  97.88566381 -52.10127768 -52.16286538\n",
            " 119.0254584  -52.1542482  -52.11516058 -52.09353589 119.30616958\n",
            "   7.24444908  16.12599039 -52.20171729 -52.23775891  34.73901108\n",
            " -52.17138847 -52.17845484  99.38637864  79.00980223  80.46361869\n",
            "  97.05876099  49.52492887  42.95022527  22.8354108  -52.25384411\n",
            " -39.5565971  -52.20378765 -52.17470059  96.29804527 -52.22218304\n",
            "  95.24555009 -52.25479115  26.21029813  96.71053005 -52.24146416\n",
            "  93.2970395  -52.1247899  -52.20936724  91.37989657  97.2499914\n",
            " -52.14570106  85.83153098 -52.25375662 -52.27133577  91.26674398\n",
            "  27.66119863  46.54955357  16.16333803  30.34325503  31.3794721\n",
            "  45.47124737 119.12964275 -52.25287636 -52.29097918  14.14980228\n",
            "  29.77412824 -52.70991475 -52.42864776 -52.42341456 -46.87236629\n",
            " -52.67709446  18.6685599  -52.86343927 -52.41211736 -52.38186036]\n",
            "New Prediction: [array([[-49.94035325]]), array([[63.39397972]]), array([[64.45363358]]), array([[-50.07264315]]), array([[-50.31374565]]), array([[57.99990632]]), array([[56.60684357]]), array([[-49.75639924]]), array([[53.76023956]]), array([[-49.39972783]]), array([[-50.21969555]]), array([[53.79646375]]), array([[64.28646249]]), array([[16.35867117]]), array([[-37.40794583]]), array([[-50.17293144]]), array([[43.57390673]]), array([[-50.07113576]]), array([[-50.29358894]]), array([[24.55881329]]), array([[62.51111081]]), array([[50.69812425]]), array([[17.20189928]]), array([[64.19039904]]), array([[-49.77107811]]), array([[-49.96467844]]), array([[-49.75319995]]), array([[57.76148009]]), array([[53.94126265]]), array([[21.06879175]]), array([[-50.3445324]]), array([[-49.79757195]]), array([[-50.25445805]]), array([[63.09943994]]), array([[-50.35142225]]), array([[62.10617027]]), array([[57.2433028]]), array([[64.2738028]]), array([[-50.35956195]]), array([[-50.19980945]]), array([[64.32366302]]), array([[-49.15977186]]), array([[-49.61512424]]), array([[-50.37141683]]), array([[61.35664333]]), array([[13.58066505]]), array([[18.2698308]]), array([[-50.36882178]]), array([[-47.70147859]]), array([[23.71971741]]), array([[-47.29901477]]), array([[-50.33582509]]), array([[57.49601824]]), array([[50.69015058]]), array([[52.67463325]]), array([[63.67096511]]), array([[48.98529302]]), array([[53.56737008]]), array([[24.12475051]]), array([[-49.95223101]]), array([[-15.08907207]]), array([[-49.35248577]]), array([[-50.29126803]]), array([[55.04955753]]), array([[-50.18277167]]), array([[54.84682186]]), array([[-50.10997631]]), array([[20.33083161]]), array([[63.38570246]]), array([[-50.14317182]]), array([[58.99894318]]), array([[-47.82285574]]), array([[-50.01103644]]), array([[63.37386815]]), array([[64.10071477]]), array([[-50.35339924]]), array([[48.88761842]]), array([[-50.24809057]]), array([[-50.09284652]]), array([[64.65735633]]), array([[22.61805481]]), array([[49.29140874]]), array([[19.57355504]]), array([[28.95762789]]), array([[19.77731696]]), array([[55.27768265]]), array([[50.04522846]]), array([[-50.32556274]]), array([[-50.13862665]]), array([[18.51771582]]), array([[19.97186246]]), array([[-50.35112332]]), array([[-50.35188754]]), array([[-48.59680762]]), array([[-28.17844215]]), array([[-50.24282509]]), array([[19.34652927]]), array([[-50.28033853]]), array([[-49.31536309]]), array([[-50.23800319]])]\n",
            "Actual: [-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
            " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
            "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
            "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
            "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
            "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
            " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
            "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
            " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
            "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
            "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
            "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
            "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
            "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
            "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
            "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
            " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
            "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
            " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
            "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
            "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
            "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
            " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
            " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
            "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n",
            "Average MSE on test data 106.67989184174861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion on 2.6: \n",
        "In this experiment, I used ten neurons in the hidden layer and one neuron in the output layer, and various learning rate=0.001 also used backpropagation 100 times. This experiment analyzes on different hidden layers that give us different losses. We summarize different losses in the following: \n",
        "\n",
        "\n",
        "1.   Number of neuron in hidden layer =1. Then Average MSE on training data 2801.8547788795295. Average MSE on training data in the new prediction 25.109119196208656.  Also Average MSE on test data 66.13005992069507. \n",
        "\n",
        "2.   The number of neurons in the hidden layer =3. Average MSE on training data 1106.633393268965; average MSE on training data in the new prediction 21.348947565986478; average MSE on test data 76.12035255535311.  \n",
        "\n",
        "3. The number of neurons in the hidden layer =5. Average MSE on training data 991.015965651613; Least MSE on training data is 579.2943147455989 at index 97 Average MSE on training data in the new prediction 18.582364286983324. Average MSE on test data 117.51977948853776\n",
        "\n",
        "\n",
        "4. The number of neurons in the hidden layer =7. Average MSE on training data 1446.2881800555863; Least MSE on training data is 636.3039525469499 at index 96 Average MSE on training data in the new prediction 11.720380661127644. Average MSE on test data 103.35053371245677.\n",
        "\n",
        "5. The number of neurons in the hidden layer =9. Average MSE on training data 1397.698641608274; Least MSE on training data is 616.102760172193 at index 95 Average MSE on training data in the new prediction 6.174490863586255. Average MSE on test data 98.99195856404027.\n",
        "\n",
        "6. The number of neurons in the hidden layer =10. Average MSE on training data 1303.7301340309966; Least MSE on training data is 468.287736536344 at index 91 Average MSE on training data in the new prediction 7.206709677720289. Average MSE on test data 106.67989184174861. \n",
        "\n",
        "\n",
        "\n",
        "The above experiment concludes that the losses decrease if we increase the neurons in the hidden layer. "
      ],
      "metadata": {
        "id": "rSy76bIsmmjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2.7"
      ],
      "metadata": {
        "id": "iZ9vytEQqivJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2(7) a,b and c: \n",
        "\n",
        "Using the hidden layer activation function $g(z)=z)$\n",
        "\n",
        "he Mean Square Error loss function \n",
        "\n",
        "$L= \\frac{1}{2N} \\sum_{i=1}^N (\\hat{y}-y)^2$, where $\\hat{y}$ and $y$ are the predicted and observed values respectively. \n",
        "\n",
        "First consider the hidden layer activation function $g(z)=z$, where $g'(z)=1$. \n",
        "\n",
        "Moreover, the  activation function $g(z)=z$ for the output layer. \n",
        "\n",
        "$Z_1=W_1x+b_1$, where $a_1=g(Z_1)$ and $Z_2=W_2 a_1+b_2$, where $a_2=g(Z_2)=\\hat{y}$. \n",
        "\n",
        "Now $\\frac{\\partial L}{\\partial W_2}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial W_2}\n",
        "=(\\hat{y}-y) \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial w_2}=(\\hat{y}-y).1. a_1=\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}-y).a_1^T$, since $\\frac{\\partial \\hat{y}}{\\partial z_2} =\\frac{\\partial g{z_2}}{\\partial z_2}=1$ and $\\frac{\\partial z_2}{\\partial w_2}=a_1$.   \n",
        "We then basically have $\\frac{\\partial L}{\\partial W_2}= (\\hat{y}-y).a_1^T$. \n",
        "\n",
        "Now $\\frac{\\partial L}{\\partial W_1}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial W_1}=(\\hat{y}-y) .1. W_2. (1).x= (\\hat{y}-y) W_2 x^T$.   \n",
        "\n",
        "Similarly, $\\frac{\\partial L}{\\partial b_2}=(\\hat{y}-y) \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial b_2}=(\\hat{y}-y) .1. 1= (\\hat{y}-y)$, \n",
        "\n",
        "and \n",
        "\n",
        "$\\frac{\\partial L}{\\partial b_1}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial b_1}= (\\hat{y}-y) .1. W_2.  (1).x=(\\hat{y}-y) W_2$.\n",
        "\n",
        "\n",
        "Finally, update the weights and bias by the following:\n",
        "\n",
        "$W_i=W_i -\\alpha \\frac{\\partial L}{\\partial W_i}$ and $b_i=b_i -\\alpha \\frac{\\partial L}{\\partial b_i}$\n",
        "\n",
        "where $i$ refers to the $i$th layer. We will repeat until convergence.  Yes, in this case we need to change the update rule. \n",
        "\n",
        "\n",
        "Yes, we will need to change the dimension of the update rule. \n",
        "\n",
        "We need to change the activation function and the dimesnsion in the update rules.  "
      ],
      "metadata": {
        "id": "f-bqg0mawulg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import legend\n",
        "##final revised  \n",
        "\n",
        "#creating neural networks for two layers \n",
        "\n",
        "#Setting the hyperparameters \n",
        "N=X_train.shape[0]\n",
        "h_1 = 10 # Number of neurons\n",
        "learning_rate = 0.001\n",
        "num_iters = 100\n",
        "h_o = 1              # output_layer_neurons\n",
        "f = X_train.shape[1] # number of feature in the traing and test data\n",
        "\n",
        "# make the iteration into a list\n",
        "iter_list = [] \n",
        "for i in range(num_iters):\n",
        "  iter_list.append(i)\n",
        "\n",
        "# Randomly initialize weight w1,w2 and bias terms b1, b2\n",
        "w1 = np.random.rand(h_1,f)\n",
        "b1 = np.random.rand(h_1,1)\n",
        "w2 = np.random.rand(h_o,h_1) \n",
        "b2 = np.random.rand(h_o,1)\n",
        "\n",
        "# Define the training set and test set \n",
        "x=X_train\n",
        "y=Y_train\n",
        "\n",
        "# Define the empty set to strore the many updates \n",
        "\n",
        "mse_list = []\n",
        "avg_mse_list = []\n",
        "mse_list_alt = []\n",
        "accuracy_list = []\n",
        "avg_mse_list = []\n",
        "truep_list = []\n",
        "truenp_list = []\n",
        "\n",
        "\n",
        "# Record weight and bias values \n",
        "w1_list = []\n",
        "w2_list = []\n",
        "b1_list= []\n",
        "b2_list = []\n",
        "a2_new_list = []\n",
        "#Working for the generalization for the neural networks\n",
        "f=x.shape[1]\n",
        "\n",
        "\n",
        "# Define the activation function for the output layer \n",
        "def linear_fun(z,a,b):\n",
        "  g2=a*z+b\n",
        "  return g2\n",
        "\n",
        "for i in range(x.shape[0]):\n",
        "  y_pred = []\n",
        "  mse_j_collect = []\n",
        "  #avg_mse_list = []\n",
        "  #truep_list = []\n",
        "  #truenp_list = []\n",
        "  #accuracy_list = []\n",
        "  for j in range(x.shape[0]):\n",
        "    z1 = np.dot(w1,x[j].reshape(f,1))+b1\n",
        "    a1 = linear_fun(z1,1,0)\n",
        "    z2 = np.dot(w2,a1)+b2\n",
        "    a2 = linear_fun(z2,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "    # want to store this predicted score in the y_pred\n",
        "    y_pred.append(a2)\n",
        "\n",
        "\n",
        "    # calculate the accuaracy\n",
        "    #if y[j]== a2 or np.round(a2,0):\n",
        "     # truep_list.append(1)\n",
        "    #else:\n",
        "     # truenp_list.append(1)\n",
        "       \n",
        "    #y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten() is also useful\n",
        "    #Using Backprop update the weights and bias terms \n",
        "    # Backward pass\n",
        "    dL_dw = (a2-y[j])\n",
        "  \n",
        "    # update weights\n",
        "    w2 = w2-(learning_rate)*dL_dw*a1.T  # check the matrix \n",
        "    w1 = w1-(learning_rate)*dL_dw*np.dot(w2,x[1].reshape(2,1).T)\n",
        "    b2 = b2-(learning_rate)*dL_dw  # check the matrix \n",
        "    b1 = b1-(learning_rate)*dL_dw*w2.T\n",
        "\n",
        "    # mse calculate\n",
        "    mse_j_ind=(a2-y[j])**2\n",
        "    mse_j_collect.append(mse_j_ind)\n",
        "\n",
        "\n",
        "  d=sum(mse_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "  mse_list_alt.append(d)\n",
        "  w1_list.append(w1)\n",
        "  w2_list.append(w2)\n",
        "  b1_list.append(b1)\n",
        "  b2_list.append(b2)\n",
        "\n",
        "  # update the y_pred in each epoch \n",
        "  y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten()\n",
        "  mse_ = np.sum((y_pred-y)**2)/(2*x.shape[0])\n",
        "  mse_list.append(mse_)\n",
        "  #accuary_ind_list=len(truep_list)/100\n",
        "  #accuracy_list.append(accuary_ind_list)\n",
        "\n",
        "\n",
        "# avg mse calculation using the training data\n",
        "avg_mse_list_alt=np.sum(mse_list_alt)/x.shape[0]\n",
        "print('Alternative option: Average MSE on training data',avg_mse_list_alt)\n",
        "avg_mse_list=np.sum(mse_list)/x.shape[0]\n",
        "print('Average MSE on training data',avg_mse_list) # Both ways give us the same average MSE on training data\n",
        "plt.plot(iter_list, mse_list)\n",
        "plt.xlabel('Number of iteration')\n",
        "plt.ylabel('Mean square Loss')\n",
        "plt.title('Number of iteration Vs. Mean square Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# using the test data we will calculate the errorn on the \n",
        "\n",
        "# To do this, we choose the weights that do well on your training set and can generalize well. \n",
        "## This usually is the weights producing the least MSE on training data. Ref: Dr. Sathya\n",
        "#For future reference, this is the default for any and all ML algorithms. \n",
        "## We should always pick the model with the best training accuracy and error. Ref: Dr. Sathya\n",
        "#First, we will see how find the best weights that gives least MSE and best accuracy on training data fir the model predictors\n",
        "# Perhaps, we don't compute the accuracy so we only consider least MSE\n",
        "\n",
        "minimum = mse_list[0]\n",
        "index = 0\n",
        "for i in range(len(mse_list)):\n",
        "  if mse_list[i] < minimum:\n",
        "    minimum = mse_list[i]\n",
        "    index= i\n",
        "print('Least MSE on training data is', minimum,'at index',index) \n",
        "\n",
        "\n",
        "# Fwd Pass with new weights\n",
        "y_pred_train_new = []\n",
        "mse_train_new_list =[]\n",
        "mse_train_new_ind = []\n",
        "a2_train_new_list = []\n",
        "mse_new_j_collect = []\n",
        "mse_new_list_alt = []\n",
        "w1_new=w1_list[index]\n",
        "w2_new=w2_list[index]\n",
        "b1_new=b1_list[index]\n",
        "b2_new=b2_list[index]\n",
        "for j in range (100):\n",
        "  z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "  a1_new = sigmoid_fun(z1_new)\n",
        "  z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "  a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "  a2_new_list.append(a2_new)\n",
        "  y_pred_train_new.append(a2)\n",
        "  \n",
        "# mse calculate\n",
        "  mse_new_j_ind=(a2_new-y[j])**2\n",
        "  mse_new_j_collect.append(mse_new_j_ind)\n",
        "\n",
        "\n",
        "d=sum(mse_new_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "mse_new_list_alt.append(d)\n",
        "\n",
        "# avg mse calculation using the training data\n",
        "avg_mse_new_list_alt=np.sum(mse_new_list_alt)/x.shape[0]\n",
        "print('Average MSE on training data in the new prediction',avg_mse_new_list_alt)\n",
        "#plt.plot(iter_list, mse_new_j_collect)\n",
        "#plt.xlabel('Number of iteration')\n",
        "#plt.ylabel('Mean square Loss')\n",
        "#plt.title('Number of iteration Vs. Mean square Loss')\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Initial Prediction\", y_pred, \"New Prediction:\", a2_new_list, \"Actual:\", y)\n",
        "#print(\"Initial Prediction\",        \"New Prediction:\",       \"Actual:\")\n",
        "#print( y_pred, a2_new_list, y)\n",
        "\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"New Prediction:\",a2_new_list)\n",
        "print(\"Actual:\",y)\n",
        "\n",
        "# alternative way to print not perfect for the moment\n",
        "#for a,b,c in zip(y_pred[::100],a2_new_list[1::100],y[2::100]):\n",
        "  #print '{:<30}{:<30}{:<}'.format(a,b,c)\n",
        "\n",
        "\n",
        "\n",
        "# Analysis on the Test data \n",
        "#X_test\n",
        "#Y_test\n",
        "mse_test_list =[]\n",
        "mse_list_ind = []\n",
        "a2_test_list = []\n",
        "# Fwd Pass with new weights\n",
        "w1_new=w1_list[index]\n",
        "w2_new=w2_list[index]\n",
        "b1_new=b1_list[index]\n",
        "b2_new=b2_list[index]\n",
        "for j in range (X_test.shape[0]):\n",
        "  z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "  a1_new = sigmoid_fun(z1_new)\n",
        "  z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "  a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "  a2_test_list.append(a2_new)\n",
        "  mse_j_test=(a2-y[j])**2\n",
        "  mse_list_ind.append(mse_j_test)\n",
        "\n",
        "dd=sum(mse_list_ind)/(2*X_test.shape[0]) # mean square error in each iteration\n",
        "mse_test_list.append(dd)\n",
        "avg_mse_test_list=np.sum(mse_test_list)/X_test.shape[0]\n",
        "print('Average MSE on test data',avg_mse_test_list) "
      ],
      "metadata": {
        "id": "72HsW-RVxjP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2(7) : a, b, and c\n",
        "\n",
        "Using the hidden layer activation function $g(z)=tanh(z)$\n",
        "\n",
        "The Mean Square Error loss function \n",
        "\n",
        "$L= \\frac{1}{2N} \\sum_{i=1}^N (\\hat{y}-y)^2$, where $\\hat{y}$ and $y$ are the predicted and observed values respectively. \n",
        "\n",
        "First consider the hidden layer activation function $g(z)=tanh(z)=\\frac{2}{1+e^{-2z}}-1$, where $g'(z)=(1-(g(z))^2)$. \n",
        "\n",
        "Moreover, the  activation function $g(z)=z$ for the output layer. \n",
        "\n",
        "$Z_1=W_1x+b_1$, where $a_1=g(Z_1)$ and $Z_2=W_2 a_1+b_2$, where $a_2=g(Z_2)=\\hat{y}$. \n",
        "\n",
        "Now $\\frac{\\partial L}{\\partial W_2}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial W_2}\n",
        "=(\\hat{y}-y) \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial w_2}=(\\hat{y}-y).1. a_1=\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}-y).a_1^T$, since $\\frac{\\partial \\hat{y}}{\\partial z_2} =\\frac{\\partial g{z_2}}{\\partial z_2}=1$ and $\\frac{\\partial z_2}{\\partial w_2}=a_1$.   \n",
        "We then basically have $\\frac{\\partial L}{\\partial W_2}= (\\hat{y}-y).a_1^T$. \n",
        "\n",
        "Now $\\frac{\\partial L}{\\partial W_1}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial W_1}=(\\hat{y}-y) .1. W_2. (1-(g(z))^2).x= (\\hat{y}-y) W_2 (1-(g(z))^2)x^T$, \n",
        "where $g(z)=tanh(z)=\\frac{2}{1+e^{-2z}}-1$.  \n",
        "\n",
        "Similarly, $\\frac{\\partial L}{\\partial b_2}=(\\hat{y}-y) \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial b_2}=(\\hat{y}-y) .1. 1= (\\hat{y}-y)$, \n",
        "\n",
        "and \n",
        "\n",
        "$\\frac{\\partial L}{\\partial b_1}=\\frac{\\partial L}{\\partial a_2} \\frac{\\partial a_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial b_1}= (\\hat{y}-y) .1. W_2.  (1-(g(z))^2).x=(\\hat{y}-y) W_2 (1-(g(z))^2)$, where $g(z)=tanh(z)=\\frac{2}{1+e^{-2z}}-1$.  \n",
        "\n",
        "\n",
        "Finally, update the weights and bias by the following:\n",
        "\n",
        "$W_i=W_i -\\alpha \\frac{\\partial L}{\\partial W_i}$ and $b_i=b_i -\\alpha \\frac{\\partial L}{\\partial b_i}$\n",
        "\n",
        "where $i$ refers to the $i$th layer. We will repeat until convergence.  Yes, in this case we need to change the update rule. \n",
        "\n",
        "Yes, we will need to change the dimension of the update rule. \n",
        "\n",
        "We need to change the activation function and the dimesnsion in the update rules.  "
      ],
      "metadata": {
        "id": "NGpF2ykhtWgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import legend\n",
        "##final revised  \n",
        "\n",
        "#creating neural networks for two layers \n",
        "\n",
        "#Setting the hyperparameters \n",
        "N=X_train.shape[0]\n",
        "h_1 = 10 # Number of neurons\n",
        "learning_rate = 0.001\n",
        "num_iters = 100\n",
        "h_o = 1              # output_layer_neurons\n",
        "f = X_train.shape[1] # number of feature in the traing and test data\n",
        "\n",
        "# make the iteration into a list\n",
        "iter_list = [] \n",
        "for i in range(num_iters):\n",
        "  iter_list.append(i)\n",
        "\n",
        "# Randomly initialize weight w1,w2 and bias terms b1, b2\n",
        "w1 = np.random.rand(h_1,f)\n",
        "b1 = np.random.rand(h_1,1)\n",
        "w2 = np.random.rand(h_o,h_1) \n",
        "b2 = np.random.rand(h_o,1)\n",
        "\n",
        "# Define the training set and test set \n",
        "x=X_train\n",
        "y=Y_train\n",
        "\n",
        "# Define the empty set to strore the many updates \n",
        "\n",
        "mse_list = []\n",
        "avg_mse_list = []\n",
        "mse_list_alt = []\n",
        "accuracy_list = []\n",
        "avg_mse_list = []\n",
        "truep_list = []\n",
        "truenp_list = []\n",
        "\n",
        "\n",
        "# Record weight and bias values \n",
        "w1_list = []\n",
        "w2_list = []\n",
        "b1_list= []\n",
        "b2_list = []\n",
        "a2_new_list = []\n",
        "#Working for the generalization for the neural networks\n",
        "f=x.shape[1]\n",
        "\n",
        "# Define the sigmoid function for the hidden layer\n",
        "def tanh_fun(z):\n",
        "  g1=2/(1+np.exp(-2z)) -1\n",
        "  return g1\n",
        "\n",
        "# Define the activation function for the output layer \n",
        "def linear_fun(z,a,b):\n",
        "  g2=a*z+b\n",
        "  return g2\n",
        "\n",
        "for i in range(x.shape[0]):\n",
        "  y_pred = []\n",
        "  mse_j_collect = []\n",
        "  #avg_mse_list = []\n",
        "  #truep_list = []\n",
        "  #truenp_list = []\n",
        "  #accuracy_list = []\n",
        "  for j in range(x.shape[0]):\n",
        "    z1 = np.dot(w1,x[j].reshape(f,1))+b1\n",
        "    a1 = tanh_fun(z)(z1)\n",
        "    z2 = np.dot(w2,a1)+b2\n",
        "    a2 = linear_fun(z2,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "    # want to store this predicted score in the y_pred\n",
        "    y_pred.append(a2)\n",
        "\n",
        "\n",
        "    # calculate the accuaracy\n",
        "    #if y[j]== a2 or np.round(a2,0):\n",
        "     # truep_list.append(1)\n",
        "    #else:\n",
        "     # truenp_list.append(1)\n",
        "       \n",
        "    #y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten() is also useful\n",
        "    #Using Backprop update the weights and bias terms \n",
        "    # Backward pass\n",
        "    dL_dw = (a2-y[j])\n",
        "  \n",
        "    # update weights\n",
        "    w2 = w2-(learning_rate)*dL_dw*a1.T  # check the matrix \n",
        "    w1 = w1-(learning_rate)*dL_dw*np.dot(w2,a1)*np.dot(1-a1,x[1].reshape(2,1).T)\n",
        "    b2 = b2-(learning_rate)*dL_dw  # check the matrix \n",
        "    b1 = b1-(learning_rate)*dL_dw*w2.T*np.dot(a1.T,1-a1)\n",
        "\n",
        "    # mse calculate\n",
        "    mse_j_ind=(a2-y[j])**2\n",
        "    mse_j_collect.append(mse_j_ind)\n",
        "\n",
        "\n",
        "  d=sum(mse_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "  mse_list_alt.append(d)\n",
        "  w1_list.append(w1)\n",
        "  w2_list.append(w2)\n",
        "  b1_list.append(b1)\n",
        "  b2_list.append(b2)\n",
        "\n",
        "  # update the y_pred in each epoch \n",
        "  y_pred=np.array(y_pred).ravel() # ravel() removes brackets from arrays; y_pred=np.array(y_pred).flatten()\n",
        "  mse_ = np.sum((y_pred-y)**2)/(2*x.shape[0])\n",
        "  mse_list.append(mse_)\n",
        "  #accuary_ind_list=len(truep_list)/100\n",
        "  #accuracy_list.append(accuary_ind_list)\n",
        "\n",
        "\n",
        "# avg mse calculation using the training data\n",
        "avg_mse_list_alt=np.sum(mse_list_alt)/x.shape[0]\n",
        "print('Alternative option: Average MSE on training data',avg_mse_list_alt)\n",
        "avg_mse_list=np.sum(mse_list)/x.shape[0]\n",
        "print('Average MSE on training data',avg_mse_list) # Both ways give us the same average MSE on training data\n",
        "plt.plot(iter_list, mse_list)\n",
        "plt.xlabel('Number of iteration')\n",
        "plt.ylabel('Mean square Loss')\n",
        "plt.title('Number of iteration Vs. Mean square Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# using the test data we will calculate the errorn on the \n",
        "\n",
        "# To do this, we choose the weights that do well on your training set and can generalize well. \n",
        "## This usually is the weights producing the least MSE on training data. Ref: Dr. Sathya\n",
        "#For future reference, this is the default for any and all ML algorithms. \n",
        "## We should always pick the model with the best training accuracy and error. Ref: Dr. Sathya\n",
        "#First, we will see how find the best weights that gives least MSE and best accuracy on training data fir the model predictors\n",
        "# Perhaps, we don't compute the accuracy so we only consider least MSE\n",
        "\n",
        "minimum = mse_list[0]\n",
        "index = 0\n",
        "for i in range(len(mse_list)):\n",
        "  if mse_list[i] < minimum:\n",
        "    minimum = mse_list[i]\n",
        "    index= i\n",
        "print('Least MSE on training data is', minimum,'at index',index) \n",
        "\n",
        "\n",
        "# Fwd Pass with new weights\n",
        "y_pred_train_new = []\n",
        "mse_train_new_list =[]\n",
        "mse_train_new_ind = []\n",
        "a2_train_new_list = []\n",
        "mse_new_j_collect = []\n",
        "mse_new_list_alt = []\n",
        "w1_new=w1_list[index]\n",
        "w2_new=w2_list[index]\n",
        "b1_new=b1_list[index]\n",
        "b2_new=b2_list[index]\n",
        "for j in range (100):\n",
        "  z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "  a1_new = sigmoid_fun(z1_new)\n",
        "  z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "  a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "  a2_new_list.append(a2_new)\n",
        "  y_pred_train_new.append(a2)\n",
        "  \n",
        "# mse calculate\n",
        "  mse_new_j_ind=(a2_new-y[j])**2\n",
        "  mse_new_j_collect.append(mse_new_j_ind)\n",
        "\n",
        "\n",
        "d=sum(mse_new_j_collect)/(2*x.shape[0]) # mean square error in each iteration\n",
        "mse_new_list_alt.append(d)\n",
        "\n",
        "# avg mse calculation using the training data\n",
        "avg_mse_new_list_alt=np.sum(mse_new_list_alt)/x.shape[0]\n",
        "print('Average MSE on training data in the new prediction',avg_mse_new_list_alt)\n",
        "#plt.plot(iter_list, mse_new_j_collect)\n",
        "#plt.xlabel('Number of iteration')\n",
        "#plt.ylabel('Mean square Loss')\n",
        "#plt.title('Number of iteration Vs. Mean square Loss')\n",
        "#plt.legend()\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Initial Prediction\", y_pred, \"New Prediction:\", a2_new_list, \"Actual:\", y)\n",
        "#print(\"Initial Prediction\",        \"New Prediction:\",       \"Actual:\")\n",
        "#print( y_pred, a2_new_list, y)\n",
        "\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"New Prediction:\",a2_new_list)\n",
        "print(\"Actual:\",y)\n",
        "\n",
        "# alternative way to print not perfect for the moment\n",
        "#for a,b,c in zip(y_pred[::100],a2_new_list[1::100],y[2::100]):\n",
        "  #print '{:<30}{:<30}{:<}'.format(a,b,c)\n",
        "\n",
        "\n",
        "\n",
        "# Analysis on the Test data \n",
        "#X_test\n",
        "#Y_test\n",
        "mse_test_list =[]\n",
        "mse_list_ind = []\n",
        "a2_test_list = []\n",
        "# Fwd Pass with new weights\n",
        "w1_new=w1_list[index]\n",
        "w2_new=w2_list[index]\n",
        "b1_new=b1_list[index]\n",
        "b2_new=b2_list[index]\n",
        "for j in range (X_test.shape[0]):\n",
        "  z1_new = np.dot(w1_new,x[j].reshape(f,1))+b1_new\n",
        "  a1_new = sigmoid_fun(z1_new)\n",
        "  z2_new= np.dot(w2_new,a1_new)+b2_new\n",
        "  a2_new = linear_fun(z2_new,1,0) # predicted value for each examples; a_2 is a number 1 by 1 \n",
        "  a2_test_list.append(a2_new)\n",
        "  mse_j_test=(a2-y[j])**2\n",
        "  mse_list_ind.append(mse_j_test)\n",
        "\n",
        "dd=sum(mse_list_ind)/(2*X_test.shape[0]) # mean square error in each iteration\n",
        "mse_test_list.append(dd)\n",
        "avg_mse_test_list=np.sum(mse_test_list)/X_test.shape[0]\n",
        "print('Average MSE on test data',avg_mse_test_list) "
      ],
      "metadata": {
        "id": "X4lpX_-Xqlvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}